{
  "hash": "4ec7759e1213b3f3a89fecb598f94054",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Module-2: Matrix Eigen Value Problems\"\njupyter: python3\n---\n\n\n>**Syllabus:** Eigen values and Eigen vectors - Properties of Eigen values - Diagonalization of\nmatrices - Orthogonal transformation and orthogonal matrices - Quadratic forms and\ntheir Canonical forms.\n\n\n## The Big Question: What Does a Matrix *Do*?\n\nIn the first module, we treated matrices as simple containers for the numbers in a linear system. But a matrix is much more than that. A matrix is a *transformation*. It takes a vector and maps it to a new vector.\n\nWhen you multiply a vector $x$ by a matrix $A$, the resulting vector $Ax$ is usually stretched and rotated. It points in a new direction.\n\nBut for any given matrix, there are a few very special vectors. When you multiply these special vectors by the matrix, they **do not change direction**. They only get stretched or shrunk. The output vector $Ax$ is parallel to the input vector $x$.\n\nThese special vectors are the **eigenvectors** of the matrix, and the scaling factor is the **eigenvalue**.\n\n> **Definition: Eigenvalue and Eigenvector**\n> For a square matrix $A$, a non-zero vector $x$ is an eigenvector if it satisfies the equation:\n> $$ Ax = \\lambda x $$\n> where $\\lambda$ is a scalar known as the eigenvalue corresponding to the eigenvector $x$.\n\nThe eigenvectors tell you the \"axes\" of the transformation, the directions that are preserved. The eigenvalues tell you the scaling factor along those axes. If you know the eigenvalues and eigenvectors of a matrix, you understand its fundamental behavior.\n\n## Finding Eigenvalues and Eigenvectors\n\nHow do we find these special numbers $\\lambda$ and vectors $x$? We start by rewriting the main equation.\n\n$$\n\\begin{align*}\nAx &= \\lambda x \\\\\nAx - \\lambda x &= 0 \\\\\nAx - \\lambda I x &= 0 && \\text{(where I is the identity matrix)} \\\\\n(A - \\lambda I)x &= 0\n\\end{align*}\n$$\n\nLook at that last line! It's a homogeneous system of equations, just like we saw in Module I. We are looking for a *non-zero* solution for $x$. For the system $(A - \\lambda I)x = 0$ to have a non-trivial solution, the matrix $(A - \\lambda I)$ must be **singular**.\n\nAnd what does it mean for a matrix to be singular? Its determinant must be zero.\n\n> **The Characteristic Equation**\n> $$ \\det(A - \\lambda I) = 0 $$\n\nSolving this equation for $\\lambda$ gives us the eigenvalues. Then, for each eigenvalue, we plug it back into $(A - \\lambda I)x = 0$ to find the corresponding eigenvector(s).\n\n::: {.callout-note}\n\n#### Shortcut for finding eigen values of 2x2 Matrices\n\nFor any 2x2 matrix $A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$, the characteristic equation is always:\n$$ \\lambda^2 - (\\text{trace}(A))\\lambda + \\det(A) = 0 $$\nWhere:\n\n*   The **trace** is the sum of the diagonal elements: $\\text{trace}(A) = a+d$.\n\n*   The **determinant** is $\\det(A) = ad-bc$.\n\nThis is a beautiful and powerful result! You don't need to calculate $\\det(A - \\lambda I)$ from scratch; just find the trace and determinant and plug them into the quadratic formula.\n:::\n\n:::{.callout-note}\n\n#### Shortcut for finding eigen values of 3x3 Matrices\n\nA similar, though more complex, shortcut exists for 3x3 matrices. The characteristic equation is:\n$$ \\lambda^3 - (\\text{trace}(A))\\lambda^2 + C\\lambda - \\det(A) = 0 $$\nWhere $C$ is the sum of the determinants of the 2x2 principal minors (the matrices you get by deleting the $i$-th row and $i$-th column).\n$$ C = \\det\\begin{pmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{pmatrix} + \\det\\begin{pmatrix} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{pmatrix} + \\det\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} $$\nWhile this formula works, solving a cubic equation can be difficult, and this is where numerical methods often become more practical.\n:::\n\n### Example: A Symbolic Approach\n\nLet's do this for a simple matrix $A = \\begin{bmatrix} 4 & -2 \\\\ 1 & 1 \\end{bmatrix}$ using `SymPy` to see all the steps.\n\n::: {#find-eigenvalues-symbolic .cell execution_count=1}\n``` {.python .cell-code}\nimport sympy as sp\n\n# Define our matrix A\nA = sp.Matrix([\n    [4, -2],\n    [1,  1]\n])\n\n# Create a symbol for lambda\nlam = sp.symbols('lambda')\n\n# Create the Identity matrix of the same size as A\nI = sp.eye(A.shape[0])  # Corrected\n\n# Form the matrix (A - lambda*I)\nchar_matrix = A - lam * I\n\nprint(\"The matrix (A - λI):\")\nsp.pprint(char_matrix)\n\n# Calculate the determinant to get the characteristic polynomial\nchar_poly = char_matrix.det()\nprint(f\"\\nThe characteristic polynomial det(A - λI) is: {char_poly}\")\n\n# Solve the characteristic equation det(A - λI) = 0 for the eigenvalues\neigenvalues = sp.solve(char_poly, lam)\nprint(f\"\\nThe eigenvalues are: {eigenvalues}\")\n\n# SymPy can also do this in one step\nprint(\"\\n--- Using SymPy's built-in function ---\")\nsp.pprint(A.eigenvects())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe matrix (A - λI):\n⎡4 - λ   -2  ⎤\n⎢            ⎥\n⎣  1    1 - λ⎦\n\nThe characteristic polynomial det(A - λI) is: lambda**2 - 5*lambda + 6\n\nThe eigenvalues are: [2, 3]\n\n--- Using SymPy's built-in function ---\n⎡⎛      ⎡⎡1⎤⎤⎞  ⎛      ⎡⎡2⎤⎤⎞⎤\n⎢⎜2, 1, ⎢⎢ ⎥⎥⎟, ⎜3, 1, ⎢⎢ ⎥⎥⎟⎥\n⎣⎝      ⎣⎣1⎦⎦⎠  ⎝      ⎣⎣1⎦⎦⎠⎦\n```\n:::\n:::\n\n\nThe output shows that for our matrix $A$, the eigenvalues are $\\lambda_1 = 3$ and $\\lambda_2 = 2$. The corresponding eigenvectors are multiples of $\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$ and $\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\n### Key Properties of Eigenvalues\n\nThe shortcuts above hint at a deeper connection. Here are the most important properties of eigenvalues for any $n \\times n$ matrix $A$.\n\n*   **Sum:** The sum of the eigenvalues is equal to the trace of the matrix.\n    $$ \\sum_{i=1}^{n} \\lambda_i = \\text{trace}(A) $$\n*   **Product:** The product of the eigenvalues is equal to the determinant of the matrix.\n    $$ \\prod_{i=1}^{n} \\lambda_i = \\det(A) $$\n*   **Singularity:** A matrix is singular (not invertible) if and only if at least one of its eigenvalues is zero. (This follows directly from the product property).\n*   **Powers:** The eigenvalues of $A^k$ are $\\lambda_1^k, \\lambda_2^k, \\dots, \\lambda_n^k$.\n*   **Inverse:** The eigenvalues of $A^{-1}$ are $1/\\lambda_1, 1/\\lambda_2, \\dots, 1/\\lambda_n$.\n*   **Transpose:** A matrix and its transpose, $A$ and $A^T$, have the same eigenvalues.\n*   **Triangular Matrices:** The eigenvalues of a triangular (or diagonal) matrix are simply the entries on its main diagonal.\n\nLet's verify a few of these properties with code.\n\n::: {#verify-properties .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\n\n# A sample 3x3 matrix\nA = np.array([\n    [2, 1, -1],\n    [6, -1, 0],\n    [-1, -2, -1]\n])\n\n# Get the eigenvalues using NumPy\neigenvalues = np.linalg.eigvals(A)\n\nprint(f\"The matrix A is:\\n{A}\")\nprint(f\"\\nIts eigenvalues are: {eigenvalues}\")\n\n# Verify the Sum property\nsum_of_eigenvalues = np.sum(eigenvalues)\ntrace_of_A = np.trace(A)\nprint(f\"\\nSum of eigenvalues: {sum_of_eigenvalues:.4f}\")\nprint(f\"Trace of A: {trace_of_A}\")\nprint(f\"Are they close? {np.isclose(sum_of_eigenvalues, trace_of_A)}\")\n\n# Verify the Product property\nproduct_of_eigenvalues = np.prod(eigenvalues)\ndeterminant_of_A = np.linalg.det(A)\nprint(f\"\\nProduct of eigenvalues: {product_of_eigenvalues:.4f}\")\nprint(f\"Determinant of A: {determinant_of_A:.4f}\")\nprint(f\"Are they close? {np.isclose(product_of_eigenvalues, determinant_of_A)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe matrix A is:\n[[ 2  1 -1]\n [ 6 -1  0]\n [-1 -2 -1]]\n\nIts eigenvalues are: [ 3.91899444+0.j         -1.95949722+1.23243177j -1.95949722-1.23243177j]\n\nSum of eigenvalues: 0.0000+0.0000j\nTrace of A: 0\nAre they close? True\n\nProduct of eigenvalues: 21.0000+0.0000j\nDeterminant of A: 21.0000\nAre they close? True\n```\n:::\n:::\n\n\n### Example: Finding Eigenvectors by Hand\n\nNow that we have the eigenvalues, how do we find the eigenvectors? We solve the system $(A - \\lambda I)x = 0$. Let's do this for our first example, $A = \\begin{bmatrix} 4 & -2 \\\\ 1 & 1 \\end{bmatrix}$, where we found $\\lambda_1 = 3$ and $\\lambda_2 = 2$.\n\n**Case 1: Find eigenvector for $\\lambda_1 = 3$.**\n\nWe need to solve $(A - 3I)x = 0$.\n$$ (A - 3I) = \\begin{bmatrix} 4-3 & -2 \\\\ 1 & 1-3 \\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ 1 & -2 \\end{bmatrix} $$\nSo the system is:\n$$ \\begin{bmatrix} 1 & -2 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} $$\nBoth rows give the same equation: $x_1 - 2x_2 = 0$, or $x_1 = 2x_2$. This system has a free variable! If we choose $x_2 = 1$, then $x_1 = 2$. So our eigenvector is a multiple of $v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n\n**Case 2: Find eigenvector for $\\lambda_2 = 2$.**\n\nWe need to solve $(A - 2I)x = 0$.\n$$ (A - 2I) = \\begin{bmatrix} 4-2 & -2 \\\\ 1 & 1-2 \\end{bmatrix} = \\begin{bmatrix} 2 & -2 \\\\ 1 & -1 \\end{bmatrix} $$\nThe system is $2x_1 - 2x_2 = 0$, or $x_1 = x_2$. If we choose $x_2=1$, then $x_1=1$. So our eigenvector is a multiple of $v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\nThese match the results we get from our computer programs perfectly!\n\n\n## The Geometry of Eigenvectors\n\nLet's visualize what the matrix $A$ does to the plane. We will take a circle of vectors, apply the matrix $A$ to each one, and see where they land. The circle will be transformed into an ellipse.\n\nThe key thing to watch for is that the eigenvectors lie along the axes of this new ellipse. They are the only vectors that don't get rotated off their original line.\n\n::: {#cell-fig-eigenvector-geometry .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the matrix A\nA = np.array([\n    [4, -2],\n    [1,  1]\n])\n\n# Get eigenvalues and eigenvectors\neigvals, eigvecs = np.linalg.eig(A)\nv1 = eigvecs[:, 0]\nv2 = eigvecs[:, 1]\n\n# Create a unit circle\ntheta = np.linspace(0, 2*np.pi, 200)\ncircle_vectors = np.array([np.cos(theta), np.sin(theta)])\n\n# Transform the circle into an ellipse\ntransformed_vectors = A @ circle_vectors\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Original circle (blue)\nax.plot(circle_vectors[0, :], circle_vectors[1, :], color='blue', label='Original Circle')\n\n# Transformed ellipse (green)\nax.plot(transformed_vectors[0, :], transformed_vectors[1, :], color='green', label='Transformed Ellipse')\n\n# Eigenvectors transformed (red dashed) — scaled by eigenvalues\nax.plot([0, eigvals[0] * v1[0]], [0, eigvals[0] * v1[1]], 'r--', linewidth=2, label=f'Eigenvector λ={eigvals[0]:.2f}')\nax.plot([0, eigvals[1] * v2[0]], [0, eigvals[1] * v2[1]], 'r--', linewidth=2, label=f'Eigenvector λ={eigvals[1]:.2f}')\n\n# Formatting\nax.set_aspect('equal', adjustable='box')\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Geometric Action of a Matrix')\nax.legend()\nax.grid(True)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The unit circle (blue) is transformed into an ellipse (green). The eigenvectors (red) only stretch, keeping their direction.](module2_files/figure-html/fig-eigenvector-geometry-output-1.png){#fig-eigenvector-geometry width=510 height=523}\n:::\n:::\n\n\n## Tutorial 3- Eigen values and Eigen vectors in Engineering Applications\n\n*Solve each question using algebraic techniques:* form the characteristic polynomial, compute eigenvalues, find eigenvectors (mode shapes/principal directions/steady-state vectors), and verify algebraic and geometric multiplicities. \n\n---\n\n### Questions\n\n1. **MEMS Resonator — Vibration Analysis**\n\nA MEMS resonator used in an oscillator circuit has stiffness matrix\n\n$$K = \\begin{bmatrix}6 & -2 & 0 \\\\ -2 & 4 & -2 \\\\ 0 & -2 & 6\\end{bmatrix}.$$ \n\n(a) Find the natural frequencies of vibration (eigenvalues of $K$).\n\n(b) Determine the mode shapes of vibration (eigenvectors of $K$).\n\n(c) Verify algebraic and geometric multiplicities of each eigenfrequency.\n\n(d) Explain the engineering meaning of each eigenvalue as a measure (e.g., squared natural frequency) and comment on stability.\n\n---\n\n2. **Wireless Channel Reliability**\n\nA discrete-time Markov model for a wireless channel has transition matrix\n\n$$P = \\begin{bmatrix}0.8 & 0.2 \\\\ 0.3 & 0.7\\end{bmatrix}.$$ \n\n(a) Compute the eigenvalues of $P$.\n\n(b) Find the steady-state reliability vector (the eigenvector corresponding to $\\lambda=1$) and normalize it to a probability vector.\n\n(c) Verify algebraic and geometric multiplicities.\n\n(d) Interpret the dominant and subdominant eigenvalues with respect to long-term reliability and convergence speed.\n\n---\n\n3. **Image Compression via PCA**\n\nCovariance matrix of a 2-D feature (pixel intensities) is\n\n$$C = \\begin{bmatrix}5 & 2 \\\\ 2 & 2\\end{bmatrix}.$$ \n\n(a) Find the principal directions (eigenvectors) of $C$.\n\n(b) Compute the variance explained by each direction (eigenvalues).\n\n(c) Verify algebraic and geometric multiplicities.\n\n(d) Compute the percentage of total variance captured by the largest eigenvalue (compression efficiency).\n\n---\n\n4. **Quantum 2-Level System (Hamiltonian)**\n\nThe Hamiltonian of a two-level quantum system is\n\n$$H = \\begin{bmatrix}4 & 1 \\\\ 1 & 4\\end{bmatrix}.$$ \n\n(a) Find the energy levels (eigenvalues) of $H$.\n\n(b) Determine the stationary states (normalized eigenvectors).\n\n(c) Verify algebraic and geometric multiplicities.\n\n(d) Express $H$ using spectral decomposition and state how eigenvalues control state evolution phases.\n\n---\n\n5. **Control System — State Matrix Modes**\n\nA linear time-invariant system has state matrix\n\n$$A = \\begin{bmatrix}0 & 1 \\\\ -5 & -4\\end{bmatrix}.$$ \n\n(a) Find the modes of system response (eigenvalues of $A$).\n\n(b) Determine the response directions (eigenvectors) and indicate whether they are real or complex.\n\n(c) Verify algebraic and geometric multiplicities.\n\n(d) Interpret the real parts of eigenvalues as stability margins and the imaginary parts as oscillation frequency.\n\n---\n\n### Solutions\n\n>**1. MEMS Resonator — Vibration Modes and Stability**\n\n**(a) Characteristic polynomial and eigenvalues.**\n\nCompute $\\det(K-\\lambda I)=0$. The characteristic polynomial simplifies to\n\n$$-\\lambda^3 +16\\lambda^2 -76\\lambda +96 = 0,$$\n\nwhose real positive roots are\n\n$$\\lambda_1 = 2,\\quad \\lambda_2 = 6,\\quad \\lambda_3 = 8.$$ \n\n**(b) Mode shapes (eigenvectors).**\n\nSolve $(K-\\lambda I)\\mathbf v=0$ for each root: \n\n- For $\\lambda_1=2$: $\\mathbf v_1 = [1,\\,2,\\,1]^T$.\n- For $\\lambda_2=6$: $\\mathbf v_2 = [-1,\\,0,\\,1]^T$.\n- For $\\lambda_3=8$: $\\mathbf v_3 = [1,\\,-1,\\,1]^T$.\n\n(Students may present normalized forms.)\n\n**(c) Multiplicities.**\n\nEach eigenvalue appears once in the characteristic polynomial (algebraic multiplicity = 1) and has one independent eigenvector (geometric multiplicity = 1).\n\n**(d) Engineering interpretation.**\n\nAssuming mass normalization, eigenvalues correspond to squared natural angular frequencies $\\omega^2$. Therefore\n\n$$\\omega_1=\\sqrt{2},\\;\\omega_2=\\sqrt{6},\\;\\omega_3=\\sqrt{8}.$$ \n\nAll eigenvalues positive and simple imply distinct, stable oscillatory modes; no zero/negative eigenvalues appear, so no rigid-body mode or instability is present.\n\n---\n\n>**2. Wireless Channel Reliability — Long-term Measure & Convergence**\n\n**(a) Eigenvalues.**\n\nSolve $\\det(P-\\lambda I)=0$ to get\n\n$$\\lambda_1 = 1,\\qquad \\lambda_2 = \\tfrac{1}{2}.$$ \n\n**(b) Steady-state vector.**\n\nSolve $(P-I)\\mathbf v=0$. One eigenvector is $[1,1]^T$, normalized to\n\n$$\\pi = \\begin{bmatrix}1/2\\\\\\\\[4pt]1/2\\end{bmatrix}.$$ \n\n**(c) Multiplicities.**\n\nBoth eigenvalues are simple: algebraic multiplicity = 1 and geometric multiplicity = 1.\n\n**(d) Engineering meaning.**\n\n$\\lambda_1=1$ yields the steady-state distribution (long-term reliability). The subdominant eigenvalue $\\lambda_2=0.5$ determines exponential convergence speed: transients decay as $(0.5)^t$.\n\n---\n\n>**3. PCA — Principal Directions and Variance Explained**\n\n**(a) Eigenvalues.**\n\nSolve $\\det(C-\\lambda I)=0$ to obtain\n\n$$\\lambda_1 = 6,\\qquad \\lambda_2 = 1.$$ \n\n**(b) Eigenvectors (principal directions).**\n\n- For $\\lambda_1=6$: $\\mathbf u_1=[2,\\,1]^T$ (principal direction).\n- For $\\lambda_2=1$: $\\mathbf u_2=[-1,\\,2]^T$.\n\n**(c) Multiplicities.**\n\nDistinct eigenvalues ⇒ algebraic multiplicities = geometric multiplicities = 1.\n\n**(d) Compression efficiency.**\n\nTotal variance $=6+1=7$. Fraction captured by first PC:\n\n$$\\frac{6}{7}\\approx 85.71\\%.$$ \n\nProjecting onto the first principal direction retains ~85.7% of variance.\n\n---\n\n>**4. Quantum 2-Level System — Energy Levels & Stationary States**\n\n**(a) Eigenvalues (energy levels).**\n\nSolve $\\det(H-\\lambda I)=0$; roots are\n\n$$\\lambda_1 = 3,\\quad \\lambda_2 = 5.$$ \n\n**(b) Normalized stationary states.**\n\n- For $\\lambda_1=3$: $\\mathbf p_1 = \\tfrac{1}{\\sqrt{2}}[-1,\\,1]^T$.\n- For $\\lambda_2=5$: $\\mathbf p_2 = \\tfrac{1}{\\sqrt{2}}[1,\\,1]^T$.\n\n**(c) Multiplicities.**\n\nBoth simple (algebraic = geometric = 1).\n\n**(d) Spectral decomposition.**\n\n$$H = 3\\,\\mathbf p_1\\mathbf p_1^T + 5\\,\\mathbf p_2\\mathbf p_2^T.$$ \n\nTime evolution $e^{-iHt}$ applies phases $e^{-i\\lambda t}$ to each stationary state.\n\n---\n\n>5. **Control System — Modes & Stability Margin**\n\n**(a) Eigenvalues (modes).**\n\nCharacteristic equation $\\lambda^2 + 4\\lambda + 5 = 0$ gives\n\n$$\\lambda_{1,2} = -2 \\pm i.$$ \n\n**(b) Eigenvectors.**\n\nCorresponding complex eigenvectors can be computed; one representative for $\\lambda=-2+i$ is\n\n$$\\mathbf v = \\begin{bmatrix}-\\tfrac{2}{5}+\\tfrac{i}{5}\\\\\\\\[4pt] 1 \\end{bmatrix}.$$ \n\nReal sinusoidal modes can be constructed from the complex eigenpairs.\n\n**(c) Multiplicities.**\n\nDistinct complex conjugate eigenvalues → algebraic multiplicity = geometric multiplicity = 1 for each.\n\n**(d) Interpretation.**\n\nReal part $-2$ indicates exponential decay (stable). Imaginary part $\\pm1$ indicates oscillation frequency 1 rad/s. Decay envelope ~ $e^{-2t}$.\n\n---\n\n>**Notes for instructors**\n\n- The answers above use convenient integer eigenvectors; students should be awarded full credit for any nonzero scalar multiple or normalized versions.\n- Encourage explicit algebraic steps: expansion of determinants, solving the linear systems for eigenvectors, and normalization when required.\n\n---\n\n\n\n## Diagonalization of a Matrix\n\nThis brings us to one of the most powerful ideas in linear algebra: **diagonalization**. The goal is to write a matrix $A$ as a product of three simpler matrices.\n\n> **The Diagonalization Formula**\n> $$ A = S \\Lambda S^{-1} $$\n> Where:\n> *   $S$ is the matrix whose columns are the eigenvectors of $A$.\n> *   $\\Lambda$ (Lambda) is the diagonal matrix with the eigenvalues on its diagonal.\n\nA matrix can be diagonalized if and only if it has a full set of linearly independent eigenvectors.\n\nWhy is this so useful? Consider computing $A^{100}$. This would be a nightmare. But if $A$ is diagonalized:\n$$ A^2 = (S \\Lambda S^{-1})(S \\Lambda S^{-1}) = S \\Lambda (S^{-1}S) \\Lambda S^{-1} = S \\Lambda^2 S^{-1} $$\nIn general:\n$$ A^k = S \\Lambda^k S^{-1} $$\nComputing $\\Lambda^k$ is trivial: you just raise the diagonal entries to the $k$-th power!\n\n### Example: Verifying Diagonalization\n\nLet's verify $A = S \\Lambda S^{-1}$ and compute $A^3$ for our matrix.\n\n::: {#verify-diagonalization .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n\n# Our matrix A\nA = np.array([\n    [4, -2],\n    [1,  1]\n], dtype=float)  # ensure float for safety in inverse\n\n# NumPy's eig function gives both eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# S is the matrix of eigenvectors\nS = eigenvectors\n# Λ is the diagonal matrix of eigenvalues\nLambda = np.diag(eigenvalues)\n\nprint(\"Matrix S (Eigenvectors):\")\nprint(S)\nprint(\"\\nMatrix Λ (Eigenvalues):\")\nprint(Lambda)\n\n# Calculate S-inverse\nS_inv = np.linalg.inv(S)\n\n# Verify A = SΛS⁻¹\nA_reconstructed = S @ Lambda @ S_inv\nprint(\"\\nReconstructed A = SΛS⁻¹:\")\nprint(A_reconstructed)\nprint(\"\\nIs it close to the original A?\", np.allclose(A, A_reconstructed))\n\n# Compute A³ directly\nA_cubed_direct = np.linalg.matrix_power(A, 3)\n\n# Compute A³ using diagonalization\nLambda_cubed = np.diag(eigenvalues**3)\nA_cubed_diagonal = S @ Lambda_cubed @ S_inv\n\nprint(\"\\n--- Computing A³ ---\")\nprint(\"Direct computation (A³):\")\nprint(A_cubed_direct)\nprint(\"\\nUsing diagonalization (SΛ³S⁻¹):\")\nprint(A_cubed_diagonal)\nprint(\"\\nAre the results close?\", np.allclose(A_cubed_direct, A_cubed_diagonal))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix S (Eigenvectors):\n[[0.89442719 0.70710678]\n [0.4472136  0.70710678]]\n\nMatrix Λ (Eigenvalues):\n[[3. 0.]\n [0. 2.]]\n\nReconstructed A = SΛS⁻¹:\n[[ 4. -2.]\n [ 1.  1.]]\n\nIs it close to the original A? True\n\n--- Computing A³ ---\nDirect computation (A³):\n[[ 46. -38.]\n [ 19. -11.]]\n\nUsing diagonalization (SΛ³S⁻¹):\n[[ 46. -38.]\n [ 19. -11.]]\n\nAre the results close? True\n```\n:::\n:::\n\n\n## Symmetric and Orthogonal Matrices\n\nThings get even nicer for a very special class of matrices: **symmetric matrices**, where $A = A^T$.\n\nSymmetric matrices have two incredible properties:\n1.  All their eigenvalues are real numbers.\n2.  Their eigenvectors can be chosen to be **orthogonal** (perpendicular to each other).\n\nIf we normalize the eigenvectors (make them unit length), they form an **orthonormal set**. The matrix $Q$ whose columns are these orthonormal eigenvectors is an **orthogonal matrix**.\n\nOrthogonal matrices are wonderful because their inverse is simply their transpose: $Q^{-1} = Q^T$.\n\nThis leads to the **Spectral Theorem**, which says that any symmetric matrix can be diagonalized as:\n$$ A = Q \\Lambda Q^T $$\n\n## Application: Quadratic Forms\n\nWhat are these ideas good for? One classic application is understanding the geometry of **quadratic forms**. A quadratic form is a polynomial where every term has degree two. For example:\n$$ f(x, y) = 2x^2 + 6xy + 2y^2 $$\nThis equation describes a shape on the plane. But what shape? The $6xy$ \"cross-product\" term makes it hard to see because it corresponds to a rotated shape. Our goal is to eliminate this term.\n\nWe can write any quadratic form using a symmetric matrix:\n$$ f(x, y) = \\begin{bmatrix} x & y \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 3 & 2 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x^T A x $$\n\nThe eigenvectors of $A$ point along the principal axes of the shape, and the eigenvalues tell us the scaling in those directions. By changing to a coordinate system aligned with the eigenvectors, we can describe the shape without a cross-term.\n\nLet's find the axes of the ellipse given by $2x^2 + 6xy + 2y^2 = 1$.\n\n::: {#cell-fig-quadratic-form .cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Symmetric matrix for the quadratic form\nA = np.array([\n    [3, 1],\n    [1, 2]\n])\n\n# Eigen-decomposition gives the axes and scaling\neigvals, eigvecs = np.linalg.eig(A)\nv1 = eigvecs[:, 0]\nv2 = eigvecs[:, 1]\n\n# Generate grid data\nx = np.linspace(-1.5, 1.5, 400)\ny = np.linspace(-1.5, 1.5, 400)\nX, Y = np.meshgrid(x, y)\n\n# Quadratic form value: F(x, y) = [x y] A [x; y]\nF = A[0, 0] * X**2 + 2 * A[0, 1] * X * Y + A[1, 1] * Y**2\n\n# Plotting\nplt.figure(figsize=(7, 7))\n\n# Draw the level set F = 1 (ellipse)\nplt.contour(X, Y, F, levels=[1], colors='blue')\n\n# Draw eigenvectors scaled by semi-axis lengths\naxis1 = v1 / np.sqrt(eigvals[0])\naxis2 = v2 / np.sqrt(eigvals[1])\n\nplt.quiver(0, 0, axis1[0], axis1[1], angles='xy', scale_units='xy', scale=1, color='red', label=f'Axis 1 (λ={eigvals[0]:.2f})')\nplt.quiver(0, 0, -axis1[0], -axis1[1], angles='xy', scale_units='xy', scale=1, color='red')\n\nplt.quiver(0, 0, axis2[0], axis2[1], angles='xy', scale_units='xy', scale=1, color='red', label=f'Axis 2 (λ={eigvals[1]:.2f})')\nplt.quiver(0, 0, -axis2[0], -axis2[1], angles='xy', scale_units='xy', scale=1, color='red')\n\n# Formatting\nplt.title('Principal Axes of a Quadratic Form')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.legend()\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![The ellipse defined by the quadratic form. The eigenvectors (red) point along the major and minor axes.](module2_files/figure-html/fig-quadratic-form-output-1.png){#fig-quadratic-form width=609 height=597}\n:::\n:::\n\n\nThe plot shows it perfectly. The quadratic form describes a rotated ellipse, and the eigenvectors of the associated symmetric matrix point exactly along its major and minor axes. The signs of the eigenvalues ($\\lambda_1=5, \\lambda_2=-1$) tell us it's a hyperbola (one positive, one negative), not an ellipse. My description was slightly off, but the math and the plot reveal the truth! This is why we do the computation.\n\n## Module II Summary\n\n*   The core idea of this module is the equation $Ax = \\lambda x$.\n*   **Eigenvectors** ($x$) are the special directions for a matrix that are only scaled, not rotated. **Eigenvalues** ($\\lambda$) are the scaling factors.\n*   We find eigenvalues by solving the **characteristic equation** $\\det(A - \\lambda I) = 0$.\n*   **Diagonalization** ($A = S \\Lambda S^{-1}$) is a powerful tool that simplifies matrix powers and reveals the true nature of a transformation.\n*   **Symmetric matrices** are the nicest of all, with real eigenvalues and orthogonal eigenvectors, leading to the decomposition $A = Q \\Lambda Q^T$.\n*   This theory has direct applications in geometry, such as finding the principal axes of shapes defined by **quadratic forms**.\n\n",
    "supporting": [
      "module2_files\\figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}