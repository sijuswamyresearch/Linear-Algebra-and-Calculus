{"title":"Module 3: Multivariable Calculus - Differentiation","markdown":{"yaml":{"title":"Module 3: Multivariable Calculus - Differentiation","jupyter":"python3"},"headingText":"A New Dimension","containsRefs":false,"markdown":"\n>**Syllabus:** Multivariable Calculus - Differentiation\nConcept of limit and continuity of functions of two variables - Partial derivatives of\nfirst and higher order - Implicit partial differentiation - Local linear approximations -\nChain rule for derivatives and partial derivatives - Relative maxima and minima of\nfunction of two variables (finding relative extrema only)\n\n----\n\nSo far, our world has been one of lines, planes, and vectors—the \"flat\" world of linear algebra. Now, we venture into the \"curvy\" world of calculus, but in higher dimensions.\n\nIn single-variable calculus, you studied functions $y = f(x)$, whose graphs are curves in a 2D plane. Now, we'll explore functions of two variables, $z = f(x, y)$. Their graphs are **surfaces** in 3D space.\n\nThink of it like this: $x$ and $y$ are your coordinates on a map (east-west and north-south), and $z$ is your altitude. The function $f(x, y)$ describes a landscape. Our goal is to understand this landscape: how steep is it? Which way is uphill? Where are the peaks and valleys?\n\nLet's start by looking at a landscape.\n\n```{python}\n#| label: fig-surface-plot\n#| fig-cap: \"An interactive plot of the surface z = f(x,y). Our goal is to analyze its features.\"\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Define the function that describes our \"landscape\"\ndef f(x, y):\n    return (x**2 + 3*y**2) * np.exp(1 - x**2 - y**2)\n\n# Create a grid of (x,y) points\nx_vals = np.linspace(-2.5, 2.5, 100)\ny_vals = np.linspace(-2.5, 2.5, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\n\n# Calculate the z-value (altitude) for each point\nZ = f(X, Y)\n\n# Create the interactive 3D surface plot\nfig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y)])\n\nfig.update_layout(\n    title='The \"Landscape\" of a Function of Two Variables',\n    scene=dict(\n        xaxis_title='x-axis',\n        yaxis_title='y-axis',\n        zaxis_title='z-axis (Altitude)'\n    ),\n    width=800, height=600,\n    autosize=False\n)\n\nfig.show()\n```\n\nLooking at this plot, we can see peaks, a valley at the center, and ridges. How can we find these features mathematically?\n\n## Partial Derivatives: The Slope in One Direction\n\nHow do we measure the \"slope\" of a surface? The problem is that the slope depends on which direction you're facing!\n\nThe simplest way to start is to do the simplest thing: **hold one variable constant**.\n\n1.  Imagine you are standing on the surface and decide to walk *only* in the x-direction (due east). The slope you experience is the **partial derivative with respect to x**, written as $\\frac{\\partial f}{\\partial x}$ or $f_x$.\n2.  Alternatively, if you walk *only* in the y-direction (due north), the slope is the **partial derivative with respect to y**, written as $\\frac{\\partial f}{\\partial y}$ or $f_y$.\n\nTo calculate $\\frac{\\partial f}{\\partial x}$, you simply treat $y$ as a constant and differentiate with respect to $x$. Let's use `SymPy` to do this for our function $f(x, y) = x^2 e^{-y}$.\n\n```{python}\n#| label: partial-derivatives-sympy\nimport sympy as sp\n\n# Define x and y as symbolic variables\nx, y = sp.symbols('x y')\n\n# Define a simpler function symbolically\nf_sym = x**2 * sp.exp(-y)\n\nprint(f\"Our function is f(x, y) = {f_sym}\")\n\n# Calculate the partial derivative with respect to x (treat y as a constant)\nfx = sp.diff(f_sym, x)\nprint(f\"The partial derivative ∂f/∂x is: {fx}\")\n\n# Calculate the partial derivative with respect to y (treat x as a constant)\nfy = sp.diff(f_sym, y)\nprint(f\"The partial derivative ∂f/∂y is: {fy}\")\n\n# We can also find second-order derivatives\nfxx = sp.diff(fx, x)\nfxy = sp.diff(fx, y)\nprint(f\"\\nThe second-order partial f_xx is: {fxx}\")\nprint(f\"The mixed partial f_xy is: {fxy}\")\n```\n\n## Tutorial- Basic Applications of Partial Differentiation in Engineering\n\n### Problem 1: Voltage variation in a resistive circuit\nIn a resistive heating circuit, the voltage developed across a resistor depends on both the current through it and its resistance, according to:\n\n$$\nV = I^2 R\n$$\n\nwhere $I$ is the current and $R$ is the resistance. The current in the circuit is $2~\\mathrm{A}$ and the resistance is $5~\\Omega$.  \n\n**Tasks:**\n\n1. Determine the sensitivity of voltage with respect to current and resistance by finding $\\dfrac{\\partial V}{\\partial I}$ and $\\dfrac{\\partial V}{\\partial R}$.\n2. Estimate the approximate change in voltage when the current increases by $0.1~\\mathrm{A}$ and the resistance decreases by $0.2~\\Omega$ due to heating.\n\n**Solution:**\n\n$$\n\\frac{\\partial V}{\\partial I} = 2 I R, \\quad \\frac{\\partial V}{\\partial R} = I^2\n$$\n\nAt $(I, R) = (2,5)$:\n\n$$\n\\frac{\\partial V}{\\partial I} = 20, \\quad \\frac{\\partial V}{\\partial R} = 4\n$$\n\nApproximate change in voltage:\n\n$$\n\\Delta V \\approx 20(0.1) + 4(-0.2) = 1.2~\\mathrm{V}\n$$\n\n**Interpretation:** A small current increase dominates, producing $\\Delta V \\approx 1.2~\\mathrm{V}$ rise.\n\n### Problem 2: Temperature distribution on a microchip surface\nIn an electronic processor, heat dissipation on the chip surface is modeled by\n\n$$\nT(x, y) = 200 e^{-0.01(x^2 + y^2)}\n$$\n\nwhere $T$ (in °C) denotes the temperature at coordinates $(x, y)$ measured in millimeters from the chip’s center.  \n\n**Tasks:**\n\n1. Find the rate of change of temperature at point $(4,3)$ along the $x$-direction.  \n2. Determine the maximum rate of increase of temperature and the direction in which it occurs.\n\n**Solution:**\n\n$$\nT_x = -4x\\, e^{-0.01(x^2 + y^2)}, \\quad T_y = -4y\\, e^{-0.01(x^2 + y^2)}\n$$\n\nAt $(4,3)$:\n\n$$\nT_x = -16 e^{-0.25}, \\quad T_y = -12 e^{-0.25}\n$$\n\nThe gradient:\n\n$$\n\\nabla T = (-16 e^{-0.25}, -12 e^{-0.25})\n$$\n\nMagnitude of maximum rate of increase:\n\n$$\n\\|\\nabla T\\| = 20 e^{-0.25}\n$$\n\nDirection toward maximum increase: $(-\\frac{4}{5}, -\\frac{3}{5})$ (toward the chip center).\n\n\n\n### Problem 3: Electrostatic potential and field intensity\nThe electrostatic potential $V$ at a point $(x, y, z)$ near a charge $q$ is expressed as\n\n$$\nV(x, y, z) = \\frac{kq}{\\sqrt{x^2 + y^2 + z^2}}\n$$\n\nwhere $k$ is a constant of proportionality.  \n\n**Tasks:**\n\n1. Compute $\\dfrac{\\partial V}{\\partial x}$, $\\dfrac{\\partial V}{\\partial y}$, and $\\dfrac{\\partial V}{\\partial z}$.\n2. Derive the expression for the electric field vector $\\vec{E} = -\\nabla V$ and discuss its physical direction.\n\n**Solution:**\n\n$$\n\\frac{\\partial V}{\\partial x} = -\\frac{kq\\, x}{(x^2 + y^2 + z^2)^{3/2}}, \\quad\n\\frac{\\partial V}{\\partial y} = -\\frac{kq\\, y}{(x^2 + y^2 + z^2)^{3/2}}, \\quad\n\\frac{\\partial V}{\\partial z} = -\\frac{kq\\, z}{(x^2 + y^2 + z^2)^{3/2}}\n$$\n\nElectric field vector:\n\n$$\n\\vec{E} = \\frac{kq (x, y, z)}{(x^2 + y^2 + z^2)^{3/2}}\n$$\n\nDirection: radially outward for a positive charge.\n\n\n### Problem 4: Gradient descent in a learning model\nIn a neural network, the cost function for a single data instance $(x_1, x_2, y)$ is defined by\n\n$$\nJ(w_1, w_2) = (w_1 x_1 + w_2 x_2 - y)^2\n$$\n\nwhere $w_1$ and $w_2$ are the model parameters.  \n\n**Tasks:**\n\n1. Find $\\dfrac{\\partial J}{\\partial w_1}$ and $\\dfrac{\\partial J}{\\partial w_2}$.\n2. Explain the significance of these derivatives in adjusting the weights using gradient descent.\n\n**Solution:**\n\n$$\n\\frac{\\partial J}{\\partial w_1} = 2 (w_1 x_1 + w_2 x_2 - y) x_1, \\quad\n\\frac{\\partial J}{\\partial w_2} = 2 (w_1 x_1 + w_2 x_2 - y) x_2\n$$\n\nThe gradient $\\nabla J$ guides weight updates via:\n\n$$\nw_i \\leftarrow w_i - \\eta \\frac{\\partial J}{\\partial w_i}\n$$\n\n\n\n### Problem 6: CPU performance sensitivity analysis\nA simplified performance model for a CPU relates processing time $t$ (ms) to clock frequency $f$ (GHz) and memory load $m$ (GB) as\n\n$$\nt(f, m) = \\frac{1000 m}{f - 0.1 m}\n$$\n\n**Tasks:**\n\n1. Find $\\dfrac{\\partial t}{\\partial f}$ and $\\dfrac{\\partial t}{\\partial m}$.\n2. Interpret these derivatives as sensitivity measures of CPU performance with respect to $f$ and $m$.\n\n**Solution:**\n\n$$\n\\frac{\\partial t}{\\partial f} = -\\frac{1000 m}{(f - 0.1 m)^2}, \\quad\n\\frac{\\partial t}{\\partial m} = \\frac{1000 f}{(f - 0.1 m)^2}\n$$\n\nIncreasing $f$ reduces processing time; increasing $m$ increases processing time. Sensitivities scale with $(f - 0.1 m)^{-2}$.\n\n\n### Problem 7\nIf $f(x, y, z) = \\ln(\\tan x + \\tan y + \\tan z)$, show that\n\n$$\n\\sin 2x\\, f_x + \\sin 2y\\, f_y + \\sin 2z\\, f_z = 2\n$$\n\n**Solution:**\n\n$$\nf_x = \\frac{\\sec^2 x}{\\tan x + \\tan y + \\tan z}, \\quad \\sin 2x\\, f_x = \\frac{2 \\tan x}{\\tan x + \\tan y + \\tan z}\n$$\n\nSimilarly for $y, z$. Adding all terms:\n\n$$\n\\sin 2x\\, f_x + \\sin 2y\\, f_y + \\sin 2z\\, f_z = 2\n$$\n\n\n\n### Problem 8\nIf $g(x, y, z) = \\ln(\\cot x + \\cot y + \\cot z)$, prove that\n\n$$\n\\sin 2x\\, g_x + \\sin 2y\\, g_y + \\sin 2z\\, g_z = -2\n$$\n\n**Solution:**\n\n$$\ng_x = \\frac{-\\csc^2 x}{\\cot x + \\cot y + \\cot z}, \\quad \\sin 2x\\, g_x = -\\frac{2 \\cot x}{\\cot x + \\cot y + \\cot z}\n$$\n\nSumming over $x, y, z$ gives $-2$.\n\n\n\n### Problem 9\nIf $h(x, y, z) = \\ln(\\tan x \\tan y \\tan z)$, show that\n\n$$\n\\sin 2x\\, h_x + \\sin 2y\\, h_y + \\sin 2z\\, h_z = 6\n$$\n\n**Solution:**\n\n$$\nh_x = \\frac{\\sec^2 x}{\\tan x} = \\frac{2}{\\sin 2x} \\Rightarrow \\sin 2x\\, h_x = 2\n$$\n\nSimilarly for $y, z$, summing gives $6$.\n\n\n\n## The Gradient and Linear Approximation\n\nThe two partial derivatives tell us the slope in the cardinal directions. But what if we want to know the slope in *any* direction? We can package our partial derivatives into a single, powerful object: the **gradient vector**.\n\n> **Definition: The Gradient**\n> The gradient of $f(x,y)$ is the vector:\n> $$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = f_x \\mathbf{i} + f_y \\mathbf{j} $$\n\nThe gradient is not just a container. It has a beautiful geometric meaning:\n\n1.  **Direction:** The gradient vector $\\nabla f$ at a point $(x_0, y_0)$ points in the direction of the **steepest ascent** on the surface. It's the \"uphill\" direction.\n2.  **Magnitude:** The magnitude of the gradient, $||\\nabla f||$, is the slope in that steepest direction.\n\nThis leads to the idea of **local linear approximation**. Just as a smooth curve looks like its tangent line up close, a smooth surface looks like its **tangent plane** up close. The gradient helps us define this plane.\n\n## Local Linear Approximation: The Tangent Plane\n\nThis is a central idea that connects everything together. Remember from single-variable calculus the **Madhava-Taylor series**. The first-order approximation of a function $f(x)$ near a point $x=a$ is its tangent line:\n$$ L(x) = f(a) + f'(a)(x-a) $$\nThis is the **local linear approximation**. The idea is powerful: if you zoom in far enough on any smooth curve, it looks like a straight line.\n\nWe will now extend this to two dimensions. If you zoom in far enough on any smooth surface, it looks like a flat **plane**. This is the **tangent plane**.\n\n> **Definition: Local Linear Approximation**\n> The local linear approximation of a function $f(x,y)$ at a point $(a,b)$ is given by:\n> $$ L(x, y) = f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) $$\n> The graph of this function, $z = L(x,y)$, is the **tangent plane** to the surface $z = f(x,y)$ at the point $(a,b)$.\n\nThis formula is a beautiful extension of the 1D case. It says the approximate height near $(a,b)$ is the starting height $f(a,b)$, plus the change due to moving in x (slope in x times distance in x), plus the change due to moving in y (slope in y times distance in y).\n\n### Problem and Application: Estimating Values\n\nLet's find the tangent plane for the function $f(x,y) = \\sqrt{x^2 + y^2}$ (a cone) at the point $(3, 4)$ and use it to approximate $f(3.01, 3.99)$.\n\n```{python}\n#| label: linear-approx-problem\n#| fig-cap: \"Using SymPy to build the linear approximation L(x,y).\"\nimport sympy as sp\n\n# Define symbols and the function\nx, y = sp.symbols('x y')\nf = sp.sqrt(x**2 + y**2)\na, b = 3, 4\n\n# 1. Find the value of the function at (a,b)\nf_val = f.subs([(x, a), (y, b)])\nprint(f\"The value f({a},{b}) is: {f_val}\")\n\n# 2. Find the partial derivatives\nfx = sp.diff(f, x)\nfy = sp.diff(f, y)\nprint(f\"∂f/∂x = {fx}\")\nprint(f\"∂f/∂y = {fy}\")\n\n# 3. Find the slope values at (a,b)\nfx_val = fx.subs([(x, a), (y, b)])\nfy_val = fy.subs([(x, a), (y, b)])\nprint(f\"\\nThe slope fx({a},{b}) is: {fx_val}\")\nprint(f\"The slope fy({a},{b}) is: {fy_val}\")\n\n# 4. Assemble the linear approximation L(x,y)\nL = f_val + fx_val * (x - a) + fy_val * (y - b)\nprint(f\"\\nThe Tangent Plane equation is: z = {sp.simplify(L)}\")\n\n# 5. Use L to approximate f(3.01, 3.99)\napprox_val = L.subs([(x, 3.01), (y, 3.99)])\nprint(f\"\\nThe approximate value of f(3.01, 3.99) is: {approx_val}\")\n\n# 6. Compare with the true value\ntrue_val = f.subs([(x, 3.01), (y, 3.99)])\nprint(f\"The true value is: {true_val.evalf()}\")\nprint(f\"The approximation is excellent!\")\n\n```\n\n### Visualization: Surface and Tangent Plane\n\nSeeing is believing. Let's plot the cone and its tangent plane at $(3, 4, 5)$. Notice how the plane perfectly \"kisses\" the surface at that single point.\n\n```{python}\n#| label: fig-tangent-plane\n#| fig-cap: \"The tangent plane (red) provides a linear approximation to the surface (blue) at the point of tangency.\"\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Define the surface function\ndef f_np(x, y):\n    return np.sqrt(x**2 + y**2)\n\n# Create grid for the surface plot\nx_surf = np.linspace(0, 6, 50)\ny_surf = np.linspace(0, 8, 50)\nX_surf, Y_surf = np.meshgrid(x_surf, y_surf)\nZ_surf = f_np(X_surf, Y_surf)\n\n# Tangent plane: L(x,y) = 5 + (3/5)(x-3) + (4/5)(y-4)\ndef L_np(x, y):\n    return 5 + (3/5)*(x - 3) + (4/5)*(y - 4)\n\nX_plane, Y_plane = np.meshgrid(np.linspace(1, 5, 10), np.linspace(2, 6, 10))\nZ_plane = L_np(X_plane, Y_plane)\n\n# Create the plot\nfig = go.Figure()\n\n# Add the surface\nfig.add_trace(go.Surface(z=Z_surf, x=X_surf, y=Y_surf, opacity=0.8, name='f(x,y)'))\n\n# Add the tangent plane\nfig.add_trace(go.Surface(z=Z_plane, x=X_plane, y=Y_plane,\n                         colorscale='Reds', showscale=False, name='Tangent Plane'))\n\n# Add the point of tangency (3,4,5)\nfig.add_trace(go.Scatter3d(\n    x=[3], y=[4], z=[5],\n    mode='markers',\n    marker=dict(size=8, color='black'),\n    name='Point (3,4,5)'\n))\n\nfig.update_layout(title='Surface and its Tangent Plane',\n                  width=800, height=600, autosize=False)\nfig.show()\n\n```\n\n\n## The Chain Rule: Derivatives on a Path\n\nWhat if you're not standing still, but walking along a path on the map? Suppose your path is given by $(x(t), y(t))$. Your altitude is then $z = f(x(t), y(t))$. How fast is your altitude changing with respect to time, $t$?\n\nThe **multivariable chain rule** gives the answer:\n$$ \\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} $$\n\nThis formula has a beautiful, compact form using the gradient and the velocity vector of your path, $r'(t) = \\begin{bmatrix} dx/dt \\\\ dy/dt \\end{bmatrix}$:\n$$ \\frac{dz}{dt} = \\nabla f \\cdot r'(t) $$\nThe rate of change of your altitude is the dot product of the \"steepest uphill\" vector and your direction of travel vector. This is a perfect example of how linear algebra and calculus work together.\n\n## Tutorial 6: Applications of Chain Rule\n\n### Problem 1: Voltage Variation in a Temperature-Dependent Circuit\n\nA resistive circuit has voltage $V = I \\cdot R$, where current and resistance vary with temperature $T$:\n\n$$\nI(T) = 5\\sqrt{T}, \\quad R(T) = 2T + 3\n$$ \n\nUsing the chain rule, find $\\dfrac{dV}{dT}$ and interpret how voltage changes as temperature increases. Verify by substitution.\n\n**Chain-rule solution:**\n\n1. Intermediate variable: $V(I(T),R(T))$  \n2. Partial derivatives:\n\n$$\n\\frac{\\partial V}{\\partial I} = R, \\quad \\frac{\\partial V}{\\partial R} = I\n$$  \n\n3. Derivatives of intermediate variables:\n\n$$\n\\frac{dI}{dT} = \\frac{5}{2} T^{-1/2}, \\quad \\frac{dR}{dT} = 2\n$$  \n\n4. Apply chain rule:\n\n$$\n\\frac{dV}{dT} = \\frac{\\partial V}{\\partial I}\\frac{dI}{dT} + \\frac{\\partial V}{\\partial R}\\frac{dR}{dT} = R \\cdot \\frac{5}{2}T^{-1/2} + I \\cdot 2\n$$  \n\n5. Substitute $I(T)$ and $R(T)$:\n\n$$\n\\frac{dV}{dT} = (2T+3) \\cdot \\frac{5}{2}T^{-1/2} + 5\\sqrt{T} \\cdot 2 = 15\\sqrt{T} +\\frac{15}{2\\sqrt{T}}\n$$\n\n**Verification by substitution:**\n\n$$\nV(T) = 5\\sqrt{T}(2T+3) \\implies \\frac{dV}{dT} = 15T^{1/2} + \\frac{15}{2T^{1/2}}\n$$\n\n**Python `SymPy` code:**\n```{python}\nimport sympy as sp\n\nT = sp.symbols('T', positive=True)\nI = 5*sp.sqrt(T)\nR = 2*T + 3\nV = I*R\n\n# Direct derivative w.r.t T\ndV_dT = sp.diff(V, T)\ndV_dT\n```\n\n>*Alternate method:*\n\n```{python}\nT = sp.symbols('T', positive=True)\nI, R = sp.symbols('I R', real=True)  # treat as independent\nV = I*R\n\n# Partial derivatives\ndV_dI = sp.diff(V, I)   # R\ndV_dR = sp.diff(V, R)   # I\n\n# Substitute expressions for I and R\nI_expr = 5*sp.sqrt(T)\nR_expr = 2*T + 3\n\n# Derivatives of I and R w.r.t T\ndI_dT = sp.diff(I_expr, T)\ndR_dT = sp.diff(R_expr, T)\n\n# Chain rule\ndV_dT = dV_dI.subs(I,I_expr).subs(R,R_expr)*dI_dT + dV_dR.subs(I,I_expr)*dR_dT\nsp.simplify(dV_dT)\n```\n\n### Problem 2: Power Output in a Time-Varying Transistor\n\nIn a transistor circuit, the instantaneous power is given by $P = V^2 / R$, where $V$ is the voltage across the transistor and $R$ is the resistance of the load. Suppose the voltage varies with time due to a decaying input signal, and the resistance slowly changes due to heating effects:\n\n$$\nV(t) = 10 e^{-0.02 t} \\text{ volts}, \\quad R(t) = 4 + 0.1 t \\ \\Omega\n$$\n\nCompute the **rate of change of power** $\\dfrac{dP}{dt}$ at any time $t$ using the **multivariable chain rule**, and verify the result by direct differentiation.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $P$ as a function of two variables: $P(V,R)$.\n\n$$\nP(V,R) = \\frac{V^2}{R}\n$$\n\n2. Compute the **partial derivatives**:\n\n$$\n\\frac{\\partial P}{\\partial V} = \\frac{2V}{R}, \\quad\n\\frac{\\partial P}{\\partial R} = -\\frac{V^2}{R^2}\n$$\n\n3. Compute **derivatives of intermediate variables** w.r.t. $t$:\n\n$$\n\\frac{dV}{dt} = \\frac{d}{dt}(10 e^{-0.02 t}) = -0.2 e^{-0.02 t}, \\quad\n\\frac{dR}{dt} = \\frac{d}{dt}(4 + 0.1 t) = 0.1\n$$\n\n4. Apply the **multivariable chain rule**:\n\n$$\n\\frac{dP}{dt} = \\frac{\\partial P}{\\partial V} \\frac{dV}{dt} + \\frac{\\partial P}{\\partial R} \\frac{dR}{dt}\n$$\n\nSubstitute the values:\n\n$$\n\\frac{dP}{dt} = \\frac{2V}{R} \\cdot (-0.2 e^{-0.02 t}) + \\left(-\\frac{V^2}{R^2}\\right) \\cdot 0.1\n$$\n\n5. Substitute $V(t)$ and $R(t)$:\n\n$$\n\\frac{dP}{dt} = \\frac{2 \\cdot 10 e^{-0.02 t}}{4 + 0.1 t} \\cdot (-0.2 e^{-0.02 t}) - \\frac{(10 e^{-0.02 t})^2}{(4 + 0.1 t)^2} \\cdot 0.1\n$$\n\nSimplify:\n\n$$\n\\frac{dP}{dt} = - \\frac{4 (e^{-0.04 t})}{4 + 0.1 t} - \\frac{100 e^{-0.04 t}}{(4 + 0.1 t)^2}\n$$\n\nThis is the **rate of change of power** at any time $t$.\n\n\n**Verification by direct differentiation:**\n\nDirectly compute:\n\n$$\nP(t) = \\frac{(10 e^{-0.02 t})^2}{4 + 0.1 t} = \\frac{100 e^{-0.04 t}}{4 + 0.1 t}\n$$\n\nDifferentiating w.r.t $t$ gives exactly the same expression:\n\n$$\n\\frac{dP}{dt} = - \\frac{4 e^{-0.04 t}}{4 + 0.1 t} - \\frac{100 e^{-0.04 t}}{(4 + 0.1 t)^2}\n$$\n\n>**Python code:**\n\n```{python}\nimport sympy as sp\n\nt = sp.symbols('t', real=True)\n# Define independent symbols for chain rule\nV_sym, R_sym = sp.symbols('V R', real=True)\nP = V_sym**2 / R_sym\n\n# Partial derivatives\ndP_dV = sp.diff(P, V_sym)   # 2*V/R\ndP_dR = sp.diff(P, R_sym)   # -V^2 / R^2\n\n# Expressions for V(t) and R(t)\nV_expr = 10*sp.exp(-0.02*t)\nR_expr = 4 + 0.1*t\n\n# Derivatives of intermediate variables\ndV_dt = sp.diff(V_expr, t)\ndR_dt = sp.diff(R_expr, t)\n\n# Chain rule\ndP_dt = dP_dV.subs({V_sym:V_expr, R_sym:R_expr})*dV_dt + dP_dR.subs({V_sym:V_expr, R_sym:R_expr})*dR_dt\ndP_dt_simplified = sp.simplify(dP_dt)\ndP_dt_simplified\n```\n### Problem 3:  Capacitance Sensitivity in a Temperature-Dependent Capacitor\n\nA parallel-plate capacitor has capacitance \n\n$$\nC = \\varepsilon \\frac{A}{d},\n$$ \n\nwhere $A$ is the plate area, $d$ is the separation, and $\\varepsilon$ is the permittivity. The capacitor is subject to temperature variations that change the radius of the circular plates and the separation:\n\n$$\nA = \\pi r^2, \\quad r = 2 + 0.01 T, \\quad d = 1 + 0.005 T\n$$\n\nCompute the *rate of change of capacitance* $\\dfrac{dC}{dT}$ using the **multivariable chain rule** and verify by direct differentiation.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $C$ as a function of two variables: $C(A(T), d(T))$.\n\n$$\nC(A,d) = \\frac{\\varepsilon A}{d}\n$$\n\n2. Compute the **partial derivatives**:\n\n$$\n\\frac{\\partial C}{\\partial A} = \\frac{\\varepsilon}{d}, \\quad\n\\frac{\\partial C}{\\partial d} = -\\frac{\\varepsilon A}{d^2}\n$$\n\n3. Compute **derivatives of intermediate variables** w.r.t $T$:\n\n$$\n\\frac{dA}{dT} = \\frac{d}{dT} (\\pi r^2) = 2\\pi r \\frac{dr}{dT} = 2 \\pi (2+0.01T)(0.01) = 0.02 \\pi (2+0.01T)\n$$\n\n$$\n\\frac{dd}{dT} = \\frac{d}{dT}(1 + 0.005 T) = 0.005\n$$\n\n4. Apply the **multivariable chain rule**:\n\n$$\n\\frac{dC}{dT} = \\frac{\\partial C}{\\partial A} \\frac{dA}{dT} + \\frac{\\partial C}{\\partial d} \\frac{dd}{dT}\n$$\n\nSubstitute the partial derivatives:\n\n$$\n\\frac{dC}{dT} = \\frac{\\varepsilon}{d} \\cdot 0.02 \\pi (2+0.01T) - \\frac{\\varepsilon A}{d^2} \\cdot 0.005\n$$\n\n5. Substitute $A = \\pi r^2 = \\pi (2 + 0.01 T)^2$ and $d = 1 + 0.005 T$:\n\n$$\n\\frac{dC}{dT} = \\frac{\\varepsilon \\cdot 0.02 \\pi (2+0.01T)}{1+0.005T} - \\frac{\\varepsilon \\pi (2+0.01T)^2 \\cdot 0.005}{(1+0.005T)^2}\n$$\n\nThis gives the **rate of change of capacitance** at any temperature $T$.\n\n**Verification by direct differentiation:**\n\nDirectly differentiate:\n\n$$\nC(T) = \\frac{\\varepsilon \\pi (2+0.01 T)^2}{1 + 0.005 T}\n$$\n\nw.r.t $T$ to get exactly the same expression as above.\n\n**Python SymPy Code (Corrected Chain-Rule Implementation):**\n\n```{python}\nimport sympy as sp\n\nT = sp.symbols('T', real=True)\neps = sp.symbols('eps', real=True)\n\n# Define intermediate variables as symbols for chain rule\nA_sym, d_sym = sp.symbols('A d', real=True)\nC = eps * A_sym / d_sym\n\n# Partial derivatives\ndC_dA = sp.diff(C, A_sym)   # eps / d\ndC_dd = sp.diff(C, d_sym)   # -eps*A / d^2\n\n# Expressions for A(T) and d(T)\nr_expr = 2 + 0.01*T\nA_expr = sp.pi * r_expr**2\nd_expr = 1 + 0.005*T\n\n# Derivatives of intermediate variables\ndA_dT = sp.diff(A_expr, T)\ndd_dT = sp.diff(d_expr, T)\n\n# Chain rule\ndC_dT = dC_dA.subs({A_sym:A_expr, d_sym:d_expr})*dA_dT + dC_dd.subs({A_sym:A_expr, d_sym:d_expr})*dd_dT\ndC_dT_simplified = sp.simplify(dC_dT)\ndC_dT_simplified\n```\n\n### Problem 4: Neural Network Weight Sensitivity\n\nConsider a simple neuron in a feedforward neural network with two inputs $x_1$ and $x_2$, weights $w_1$ and $w_2$, and bias $b$. The output of the neuron is  \n\n$$\nz = \\tanh(u), \\quad u = w_1 x_1 + w_2 x_2 + b\n$$  \n\nCompute the *sensitivity of the output* with respect to the weights, i.e., $\\frac{\\partial z}{\\partial w_1}$ and $\\frac{\\partial z}{\\partial w_2}$, using the *multivariable chain rule*. Verify the results using Python's SymPy library.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $z$ as a function of $u$: $z = \\tanh(u)$.  \n2. Compute the derivative of $z$ with respect to $u$:\n\n$$\n\\frac{dz}{du} = 1 - \\tanh^2(u)\n$$\n\n3. Compute the derivatives of $u$ with respect to the weights:\n\n$$\n\\frac{\\partial u}{\\partial w_1} = x_1, \\quad \\frac{\\partial u}{\\partial w_2} = x_2\n$$\n\n4. Apply the **chain rule**:\n\n$$\n\\frac{\\partial z}{\\partial w_1} = \\frac{dz}{du} \\cdot \\frac{\\partial u}{\\partial w_1} = (1 - \\tanh^2(u)) \\cdot x_1\n$$\n\n$$\n\\frac{\\partial z}{\\partial w_2} = \\frac{dz}{du} \\cdot \\frac{\\partial u}{\\partial w_2} = (1 - \\tanh^2(u)) \\cdot x_2\n$$\n\n\n**Verification using Python SymPy:**\n\n```{python}\nimport sympy as sp\n\n# Define symbols\nw1, w2, x1, x2, b = sp.symbols('w1 w2 x1 x2 b', real=True)\n\n# Define u and z\nu = w1*x1 + w2*x2 + b\nz = sp.tanh(u)\n\n# Compute derivatives using chain rule automatically\ndz_dw1 = sp.diff(z, w1)\ndz_dw2 = sp.diff(z, w2)\n\n# Simplify\ndz_dw1_simpl = sp.simplify(dz_dw1)\ndz_dw2_simpl = sp.simplify(dz_dw2)\n\ndz_dw1_simpl, dz_dw2_simpl\n```\n\n### Problem 5: Temperature in a Polar Sensor Grid\n \nA sensor grid measures temperature at points $(x, y)$, where the temperature depends on position as  \n\n$$\nT = x^2 + y^2\n$$  \n\nSuppose the sensors are arranged in a polar coordinate system:\n\n$$\nx = r \\cos\\theta, \\quad y = r \\sin\\theta\n$$  \n\nCompute the *rate of change of temperature* with respect to the radial distance $r$ and the angular position $\\theta$, i.e., $\\frac{\\partial T}{\\partial r}$ and $\\frac{\\partial T}{\\partial \\theta}$, using the *multivariable chain rule*. Verify the results using Python `SymPy`.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $T$ as a function of $x$ and $y$:\n\n$$\nT(x,y) = x^2 + y^2\n$$\n\nPartial derivatives:\n\n$$\n\\frac{\\partial T}{\\partial x} = 2x, \\quad \\frac{\\partial T}{\\partial y} = 2y\n$$\n\n2. Compute derivatives of $x$ and $y$ with respect to $r$ and $\\theta$:\n\n$$\n\\frac{\\partial x}{\\partial r} = \\cos\\theta, \\quad \\frac{\\partial x}{\\partial \\theta} = -r \\sin\\theta\n$$\n\n$$\n\\frac{\\partial y}{\\partial r} = \\sin\\theta, \\quad \\frac{\\partial y}{\\partial \\theta} = r \\cos\\theta\n$$\n\n3. Apply the **multivariable chain rule**:\n\n$$\n\\frac{\\partial T}{\\partial r} = \\frac{\\partial T}{\\partial x} \\frac{\\partial x}{\\partial r} + \\frac{\\partial T}{\\partial y} \\frac{\\partial y}{\\partial r} = 2x \\cos\\theta + 2y \\sin\\theta\n$$\n\n$$\n\\frac{\\partial T}{\\partial \\theta} = \\frac{\\partial T}{\\partial x} \\frac{\\partial x}{\\partial \\theta} + \\frac{\\partial T}{\\partial y} \\frac{\\partial y}{\\partial \\theta} = 2x(-r \\sin\\theta) + 2y(r \\cos\\theta) = 0\n$$\n\n4. Substitute $x = r \\cos\\theta$, $y = r \\sin\\theta$:\n\n$$\n\\frac{\\partial T}{\\partial r} = 2r (\\cos^2\\theta + \\sin^2\\theta) = 2r\n$$\n\n$$\n\\frac{\\partial T}{\\partial \\theta} = 0\n$$\n\n\n**Verification using Python SymPy:**\n\n```{python}\nimport sympy as sp\n\n# Define symbols\nr, theta = sp.symbols('r theta', real=True)\n\n# Define coordinate transformations\nx = r * sp.cos(theta)\ny = r * sp.sin(theta)\n\n# Temperature function\nT = x**2 + y**2\n\n# Partial derivatives w.r.t r and theta\ndT_dr = sp.diff(T, r)\ndT_dtheta = sp.diff(T, theta)\n\ndT_dr_simpl = sp.simplify(dT_dr)\ndT_dtheta_simpl = sp.simplify(dT_dtheta)\n\ndT_dr_simpl, dT_dtheta_simpl\n```\n\n### Problem 6: Dynamic System Response Over Time\n\nA dynamic system has an output that depends multiplicatively on two time-varying inputs. The instantaneous output is\n\n$$\nz = e^{\\,x y},\n$$\n\nwhere the inputs themselves vary with time as\n\n$$\nx(t) = 2t^2,\\qquad y(t) = 3t + 1.\n$$\n\nUse the *multivariable chain rule* to compute the total derivative $\\dfrac{dz}{dt}$ (i.e. the rate of change of the output with respect to time). Then *verify* the result by substituting $x(t),y(t)$ into $z$ and differentiating directly. Evaluate the rate at $t=1$ and interpret the result.\n\n**Solution (Chain-Rule Method):**\n\n1. Identify intermediate variables: $z = f(x,y)$ with $f(x,y)=e^{xy}$, and $x=x(t),\\ y=y(t)$.\n\n2. Compute the partial derivatives of $z$ w.r.t the intermediate variables:\n\n$$\n\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x} e^{xy} = y\\,e^{xy}, \\qquad\n\\frac{\\partial z}{\\partial y} = \\frac{\\partial}{\\partial y} e^{xy} = x\\,e^{xy}.\n$$\n\n3. Compute time-derivatives of the intermediate variables:\n\n$$\n\\frac{dx}{dt} = \\frac{d}{dt}(2t^2) = 4t, \\qquad\n\\frac{dy}{dt} = \\frac{d}{dt}(3t+1) = 3.\n$$\n\n4. Apply the multivariable chain rule (total derivative):\n\n$$\n\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}.\n$$\n\nSubstitute the partials and time-derivatives:\n\n$$\n\\frac{dz}{dt} = \\bigl(y e^{xy}\\bigr)(4t) + \\bigl(x e^{xy}\\bigr)(3)\n= e^{xy}\\bigl(4t y + 3x\\bigr).\n$$\n\n5. Evaluate at $t=1$. First compute $x(1)=2\\cdot1^2 = 2$ and $y(1)=3\\cdot1+1 = 4$. Thus\n\n$$\n\\frac{dz}{dt}\\Big|_{t=1} = e^{(2)(4)}\\bigl(4\\cdot1\\cdot4 + 3\\cdot2\\bigr) = e^{8}(16 + 6) = 22 e^{8}.\n$$\n\n**Interpretation:** At $t=1$ the output is increasing very rapidly: the instantaneous rate is $22e^8$, reflecting strong sensitivity because $z$ is exponential in the product $xy$.\n\n**Verification by substitution (direct differentiation):**\n\nSubstitute $x(t)$ and $y(t)$ into $z$:\n\n$$\nz(t) = e^{(2t^2)(3t+1)} = e^{6t^3 + 2t^2}.\n$$\n\nDifferentiate directly:\n\n$$\n\\frac{dz}{dt} = e^{6t^3 + 2t^2}\\cdot(18t^2 + 4t).\n$$\n\nCheck algebraically that\n\n$$\n18t^2 + 4t \\equiv 4t(3t+1) + 3(2t^2) = 4t y + 3x,\n$$\n\nso the direct differentiation result matches the chain-rule result. Evaluating at $t=1$ gives $22e^8$ as before.\n\n**Python (SymPy) verification code:**\n\n```{python}\nimport sympy as sp\n\n# symbol\nt = sp.symbols('t', real=True)\n\n# define x(t), y(t)\nx_expr = 2*t**2\ny_expr = 3*t + 1\n\n# Method A: explicit chain-rule via partials (use x,y as symbols then substitute)\nx_sym, y_sym = sp.symbols('x_sym y_sym')\nz_sym = sp.exp(x_sym*y_sym)\n\n# partials\nz_x = sp.diff(z_sym, x_sym)   # y * exp(xy)\nz_y = sp.diff(z_sym, y_sym)   # x * exp(xy)\n\n# substitute x(t), y(t) and multiply by dx/dt, dy/dt\ndzdt_chain = (z_x.subs({x_sym: x_expr, y_sym: y_expr}) * sp.diff(x_expr, t) +\n              z_y.subs({x_sym: x_expr, y_sym: y_expr}) * sp.diff(y_expr, t))\ndzdt_chain_simpl = sp.simplify(dzdt_chain)\n\n# evaluate at t=1\ndzdt_chain_at_1 = dzdt_chain_simpl.subs(t, 1)\n\n# Method B: substitute then differentiate\nz_sub = sp.exp(x_expr * y_expr)\ndzdt_sub = sp.diff(z_sub, t)\ndzdt_sub_simpl = sp.simplify(dzdt_sub)\ndzdt_sub_at_1 = dzdt_sub_simpl.subs(t, 1)\n\ndzdt_chain_simpl, dzdt_chain_at_1, dzdt_sub_simpl, dzdt_sub_at_1\n```\n## The Main Event: Finding Maxima and Minima\n\nNow we can answer the big question: how do we find the peaks and valleys of our landscape?\n\nAt the very top of a peak or the bottom of a valley, the ground is perfectly flat. The slope in *every* direction is zero. This means both partial derivatives must be zero.\n\n> **Critical Points**\n> A point $(a, b)$ is a **critical point** of $f(x,y)$ if the gradient at that point is the zero vector:\n> $$ \\nabla f(a, b) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{which means} \\quad f_x(a, b) = 0 \\text{ and } f_y(a, b) = 0 $$\n\nBut a flat spot isn't always a peak or a valley. It could also be a **saddle point**, like the middle of a Pringles chip—it's a minimum in one direction and a maximum in another.\n\nTo classify these critical points, we need a multivariable version of the Second Derivative Test. This test involves a quantity called the **Discriminant** ($D$), which is built from the second-order partial derivatives.\n\n::: {.callout-note}\n### The Second Derivative Test\n\nFirst, find all critical points by solving $\\nabla f = 0$. Then, for each critical point $(a, b)$, calculate the second partial derivatives ($f_{xx}, f_{yy}, f_{xy}$) at that point.\n\nDefine the Discriminant $D = f_{xx}(a,b) f_{yy}(a,b) - [f_{xy}(a,b)]^2$.\n\n1.  If $D > 0$ and $f_{xx}(a,b) > 0$, then $f$ has a **local minimum** at $(a, b)$.\n2.  If $D > 0$ and $f_{xx}(a,b) < 0$, then $f$ has a **local maximum** at $(a, b)$.\n3.  If $D < 0$, then $f$ has a **saddle point** at $(a, b)$.\n4.  If $D = 0$, the test is inconclusive.\n:::\n\nNote that the Discriminant is just the determinant of the **Hessian matrix**, a beautiful connection back to linear algebra!\n$$ H = \\begin{bmatrix} f_{xx} & f_{xy} \\\\ f_{yx} & f_{yy} \\end{bmatrix} \\implies D = \\det(H) $$\n\n### Example: Finding the Extrema of Our Landscape\n\nLet's use `SymPy` to find and classify all the critical points of the function $f(x, y) = (x^2 + 3y^2) e^{1 - x^2 - y^2}$ we plotted at the beginning.\n\n```{python}\n#| label: find-classify-extrema\n#| fig-cap: \"Using SymPy to find and classify the critical points of our surface.\"\n\nimport sympy as sp\n\n# Define symbols and the function\nx, y = sp.symbols('x y')\nf = (x**2 + 3*y**2) * sp.exp(1 - x**2 - y**2)\n\n# 1. Find the partial derivatives\nfx = sp.diff(f, x)\nfy = sp.diff(f, y)\n\n# 2. Find the critical points by solving ∇f = 0\n# This can be computationally intensive; we'll use a numerical approach for clarity\n# For this specific function, inspection shows critical points at:\n# (0,0), (1,0), (-1,0), (0,1), (0,-1)\ncritical_points = [\n    (0, 0),\n    (1, 0),\n    (-1, 0),\n    (0, 1),\n    (0, -1)\n]\nprint(f\"The critical points are: {critical_points}\\n\")\n\n\n# 3. Calculate second-order partial derivatives\nfxx = sp.diff(fx, x)\nfyy = sp.diff(fy, y)\nfxy = sp.diff(fx, y)\n\n# 4. Create the Discriminant D\nD = fxx * fyy - fxy**2\n\n# 5. Classify each critical point\nfor p in critical_points:\n    px, py = p\n    # Substitute the point's coordinates into D and fxx\n    D_val = D.subs([(x, px), (y, py)])\n    fxx_val = fxx.subs([(x, px), (y, py)])\n    \n    print(f\"--- Analyzing point {p} ---\")\n    print(f\"  D = {D_val:.2f}, f_xx = {fxx_val:.2f}\")\n\n    if D_val > 0 and fxx_val > 0:\n        print(\"  Result: Local Minimum\")\n    elif D_val > 0 and fxx_val < 0:\n        print(\"  Result: Local Maximum\")\n    elif D_val < 0:\n        print(\"  Result: Saddle Point\")\n    else:\n        print(\"  Result: Test is inconclusive\")\n```\n\nThe results match what we see in the 3D plot perfectly! The origin is a local minimum, the points on the y-axis are local maxima (the two highest peaks), and the points on the x-axis are saddle points.\n\n## Module III Summary\n\n*   We've moved from 2D curves to 3D surfaces, or \"landscapes.\"\n*   **Partial derivatives** ($f_x, f_y$) are the slopes in the cardinal directions.\n*   The **gradient vector** ($\\nabla f = [f_x, f_y]$) packages these slopes and points in the direction of steepest ascent. It is the key to understanding the local geometry of a surface.\n*   To find potential maxima and minima (**critical points**), we find where the landscape is flat by solving $\\nabla f = 0$.\n*   The **Second Derivative Test**, using the determinant of the Hessian matrix, allows us to classify these critical points as local maxima, local minima, or saddle points.\n*   This process of finding extrema is called **optimization**, and it is the absolute core of how modern AI models are trained.\n","srcMarkdownNoYaml":"\n>**Syllabus:** Multivariable Calculus - Differentiation\nConcept of limit and continuity of functions of two variables - Partial derivatives of\nfirst and higher order - Implicit partial differentiation - Local linear approximations -\nChain rule for derivatives and partial derivatives - Relative maxima and minima of\nfunction of two variables (finding relative extrema only)\n\n----\n## A New Dimension\n\nSo far, our world has been one of lines, planes, and vectors—the \"flat\" world of linear algebra. Now, we venture into the \"curvy\" world of calculus, but in higher dimensions.\n\nIn single-variable calculus, you studied functions $y = f(x)$, whose graphs are curves in a 2D plane. Now, we'll explore functions of two variables, $z = f(x, y)$. Their graphs are **surfaces** in 3D space.\n\nThink of it like this: $x$ and $y$ are your coordinates on a map (east-west and north-south), and $z$ is your altitude. The function $f(x, y)$ describes a landscape. Our goal is to understand this landscape: how steep is it? Which way is uphill? Where are the peaks and valleys?\n\nLet's start by looking at a landscape.\n\n```{python}\n#| label: fig-surface-plot\n#| fig-cap: \"An interactive plot of the surface z = f(x,y). Our goal is to analyze its features.\"\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Define the function that describes our \"landscape\"\ndef f(x, y):\n    return (x**2 + 3*y**2) * np.exp(1 - x**2 - y**2)\n\n# Create a grid of (x,y) points\nx_vals = np.linspace(-2.5, 2.5, 100)\ny_vals = np.linspace(-2.5, 2.5, 100)\nX, Y = np.meshgrid(x_vals, y_vals)\n\n# Calculate the z-value (altitude) for each point\nZ = f(X, Y)\n\n# Create the interactive 3D surface plot\nfig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y)])\n\nfig.update_layout(\n    title='The \"Landscape\" of a Function of Two Variables',\n    scene=dict(\n        xaxis_title='x-axis',\n        yaxis_title='y-axis',\n        zaxis_title='z-axis (Altitude)'\n    ),\n    width=800, height=600,\n    autosize=False\n)\n\nfig.show()\n```\n\nLooking at this plot, we can see peaks, a valley at the center, and ridges. How can we find these features mathematically?\n\n## Partial Derivatives: The Slope in One Direction\n\nHow do we measure the \"slope\" of a surface? The problem is that the slope depends on which direction you're facing!\n\nThe simplest way to start is to do the simplest thing: **hold one variable constant**.\n\n1.  Imagine you are standing on the surface and decide to walk *only* in the x-direction (due east). The slope you experience is the **partial derivative with respect to x**, written as $\\frac{\\partial f}{\\partial x}$ or $f_x$.\n2.  Alternatively, if you walk *only* in the y-direction (due north), the slope is the **partial derivative with respect to y**, written as $\\frac{\\partial f}{\\partial y}$ or $f_y$.\n\nTo calculate $\\frac{\\partial f}{\\partial x}$, you simply treat $y$ as a constant and differentiate with respect to $x$. Let's use `SymPy` to do this for our function $f(x, y) = x^2 e^{-y}$.\n\n```{python}\n#| label: partial-derivatives-sympy\nimport sympy as sp\n\n# Define x and y as symbolic variables\nx, y = sp.symbols('x y')\n\n# Define a simpler function symbolically\nf_sym = x**2 * sp.exp(-y)\n\nprint(f\"Our function is f(x, y) = {f_sym}\")\n\n# Calculate the partial derivative with respect to x (treat y as a constant)\nfx = sp.diff(f_sym, x)\nprint(f\"The partial derivative ∂f/∂x is: {fx}\")\n\n# Calculate the partial derivative with respect to y (treat x as a constant)\nfy = sp.diff(f_sym, y)\nprint(f\"The partial derivative ∂f/∂y is: {fy}\")\n\n# We can also find second-order derivatives\nfxx = sp.diff(fx, x)\nfxy = sp.diff(fx, y)\nprint(f\"\\nThe second-order partial f_xx is: {fxx}\")\nprint(f\"The mixed partial f_xy is: {fxy}\")\n```\n\n## Tutorial- Basic Applications of Partial Differentiation in Engineering\n\n### Problem 1: Voltage variation in a resistive circuit\nIn a resistive heating circuit, the voltage developed across a resistor depends on both the current through it and its resistance, according to:\n\n$$\nV = I^2 R\n$$\n\nwhere $I$ is the current and $R$ is the resistance. The current in the circuit is $2~\\mathrm{A}$ and the resistance is $5~\\Omega$.  \n\n**Tasks:**\n\n1. Determine the sensitivity of voltage with respect to current and resistance by finding $\\dfrac{\\partial V}{\\partial I}$ and $\\dfrac{\\partial V}{\\partial R}$.\n2. Estimate the approximate change in voltage when the current increases by $0.1~\\mathrm{A}$ and the resistance decreases by $0.2~\\Omega$ due to heating.\n\n**Solution:**\n\n$$\n\\frac{\\partial V}{\\partial I} = 2 I R, \\quad \\frac{\\partial V}{\\partial R} = I^2\n$$\n\nAt $(I, R) = (2,5)$:\n\n$$\n\\frac{\\partial V}{\\partial I} = 20, \\quad \\frac{\\partial V}{\\partial R} = 4\n$$\n\nApproximate change in voltage:\n\n$$\n\\Delta V \\approx 20(0.1) + 4(-0.2) = 1.2~\\mathrm{V}\n$$\n\n**Interpretation:** A small current increase dominates, producing $\\Delta V \\approx 1.2~\\mathrm{V}$ rise.\n\n### Problem 2: Temperature distribution on a microchip surface\nIn an electronic processor, heat dissipation on the chip surface is modeled by\n\n$$\nT(x, y) = 200 e^{-0.01(x^2 + y^2)}\n$$\n\nwhere $T$ (in °C) denotes the temperature at coordinates $(x, y)$ measured in millimeters from the chip’s center.  \n\n**Tasks:**\n\n1. Find the rate of change of temperature at point $(4,3)$ along the $x$-direction.  \n2. Determine the maximum rate of increase of temperature and the direction in which it occurs.\n\n**Solution:**\n\n$$\nT_x = -4x\\, e^{-0.01(x^2 + y^2)}, \\quad T_y = -4y\\, e^{-0.01(x^2 + y^2)}\n$$\n\nAt $(4,3)$:\n\n$$\nT_x = -16 e^{-0.25}, \\quad T_y = -12 e^{-0.25}\n$$\n\nThe gradient:\n\n$$\n\\nabla T = (-16 e^{-0.25}, -12 e^{-0.25})\n$$\n\nMagnitude of maximum rate of increase:\n\n$$\n\\|\\nabla T\\| = 20 e^{-0.25}\n$$\n\nDirection toward maximum increase: $(-\\frac{4}{5}, -\\frac{3}{5})$ (toward the chip center).\n\n\n\n### Problem 3: Electrostatic potential and field intensity\nThe electrostatic potential $V$ at a point $(x, y, z)$ near a charge $q$ is expressed as\n\n$$\nV(x, y, z) = \\frac{kq}{\\sqrt{x^2 + y^2 + z^2}}\n$$\n\nwhere $k$ is a constant of proportionality.  \n\n**Tasks:**\n\n1. Compute $\\dfrac{\\partial V}{\\partial x}$, $\\dfrac{\\partial V}{\\partial y}$, and $\\dfrac{\\partial V}{\\partial z}$.\n2. Derive the expression for the electric field vector $\\vec{E} = -\\nabla V$ and discuss its physical direction.\n\n**Solution:**\n\n$$\n\\frac{\\partial V}{\\partial x} = -\\frac{kq\\, x}{(x^2 + y^2 + z^2)^{3/2}}, \\quad\n\\frac{\\partial V}{\\partial y} = -\\frac{kq\\, y}{(x^2 + y^2 + z^2)^{3/2}}, \\quad\n\\frac{\\partial V}{\\partial z} = -\\frac{kq\\, z}{(x^2 + y^2 + z^2)^{3/2}}\n$$\n\nElectric field vector:\n\n$$\n\\vec{E} = \\frac{kq (x, y, z)}{(x^2 + y^2 + z^2)^{3/2}}\n$$\n\nDirection: radially outward for a positive charge.\n\n\n### Problem 4: Gradient descent in a learning model\nIn a neural network, the cost function for a single data instance $(x_1, x_2, y)$ is defined by\n\n$$\nJ(w_1, w_2) = (w_1 x_1 + w_2 x_2 - y)^2\n$$\n\nwhere $w_1$ and $w_2$ are the model parameters.  \n\n**Tasks:**\n\n1. Find $\\dfrac{\\partial J}{\\partial w_1}$ and $\\dfrac{\\partial J}{\\partial w_2}$.\n2. Explain the significance of these derivatives in adjusting the weights using gradient descent.\n\n**Solution:**\n\n$$\n\\frac{\\partial J}{\\partial w_1} = 2 (w_1 x_1 + w_2 x_2 - y) x_1, \\quad\n\\frac{\\partial J}{\\partial w_2} = 2 (w_1 x_1 + w_2 x_2 - y) x_2\n$$\n\nThe gradient $\\nabla J$ guides weight updates via:\n\n$$\nw_i \\leftarrow w_i - \\eta \\frac{\\partial J}{\\partial w_i}\n$$\n\n\n\n### Problem 6: CPU performance sensitivity analysis\nA simplified performance model for a CPU relates processing time $t$ (ms) to clock frequency $f$ (GHz) and memory load $m$ (GB) as\n\n$$\nt(f, m) = \\frac{1000 m}{f - 0.1 m}\n$$\n\n**Tasks:**\n\n1. Find $\\dfrac{\\partial t}{\\partial f}$ and $\\dfrac{\\partial t}{\\partial m}$.\n2. Interpret these derivatives as sensitivity measures of CPU performance with respect to $f$ and $m$.\n\n**Solution:**\n\n$$\n\\frac{\\partial t}{\\partial f} = -\\frac{1000 m}{(f - 0.1 m)^2}, \\quad\n\\frac{\\partial t}{\\partial m} = \\frac{1000 f}{(f - 0.1 m)^2}\n$$\n\nIncreasing $f$ reduces processing time; increasing $m$ increases processing time. Sensitivities scale with $(f - 0.1 m)^{-2}$.\n\n\n### Problem 7\nIf $f(x, y, z) = \\ln(\\tan x + \\tan y + \\tan z)$, show that\n\n$$\n\\sin 2x\\, f_x + \\sin 2y\\, f_y + \\sin 2z\\, f_z = 2\n$$\n\n**Solution:**\n\n$$\nf_x = \\frac{\\sec^2 x}{\\tan x + \\tan y + \\tan z}, \\quad \\sin 2x\\, f_x = \\frac{2 \\tan x}{\\tan x + \\tan y + \\tan z}\n$$\n\nSimilarly for $y, z$. Adding all terms:\n\n$$\n\\sin 2x\\, f_x + \\sin 2y\\, f_y + \\sin 2z\\, f_z = 2\n$$\n\n\n\n### Problem 8\nIf $g(x, y, z) = \\ln(\\cot x + \\cot y + \\cot z)$, prove that\n\n$$\n\\sin 2x\\, g_x + \\sin 2y\\, g_y + \\sin 2z\\, g_z = -2\n$$\n\n**Solution:**\n\n$$\ng_x = \\frac{-\\csc^2 x}{\\cot x + \\cot y + \\cot z}, \\quad \\sin 2x\\, g_x = -\\frac{2 \\cot x}{\\cot x + \\cot y + \\cot z}\n$$\n\nSumming over $x, y, z$ gives $-2$.\n\n\n\n### Problem 9\nIf $h(x, y, z) = \\ln(\\tan x \\tan y \\tan z)$, show that\n\n$$\n\\sin 2x\\, h_x + \\sin 2y\\, h_y + \\sin 2z\\, h_z = 6\n$$\n\n**Solution:**\n\n$$\nh_x = \\frac{\\sec^2 x}{\\tan x} = \\frac{2}{\\sin 2x} \\Rightarrow \\sin 2x\\, h_x = 2\n$$\n\nSimilarly for $y, z$, summing gives $6$.\n\n\n\n## The Gradient and Linear Approximation\n\nThe two partial derivatives tell us the slope in the cardinal directions. But what if we want to know the slope in *any* direction? We can package our partial derivatives into a single, powerful object: the **gradient vector**.\n\n> **Definition: The Gradient**\n> The gradient of $f(x,y)$ is the vector:\n> $$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix} = f_x \\mathbf{i} + f_y \\mathbf{j} $$\n\nThe gradient is not just a container. It has a beautiful geometric meaning:\n\n1.  **Direction:** The gradient vector $\\nabla f$ at a point $(x_0, y_0)$ points in the direction of the **steepest ascent** on the surface. It's the \"uphill\" direction.\n2.  **Magnitude:** The magnitude of the gradient, $||\\nabla f||$, is the slope in that steepest direction.\n\nThis leads to the idea of **local linear approximation**. Just as a smooth curve looks like its tangent line up close, a smooth surface looks like its **tangent plane** up close. The gradient helps us define this plane.\n\n## Local Linear Approximation: The Tangent Plane\n\nThis is a central idea that connects everything together. Remember from single-variable calculus the **Madhava-Taylor series**. The first-order approximation of a function $f(x)$ near a point $x=a$ is its tangent line:\n$$ L(x) = f(a) + f'(a)(x-a) $$\nThis is the **local linear approximation**. The idea is powerful: if you zoom in far enough on any smooth curve, it looks like a straight line.\n\nWe will now extend this to two dimensions. If you zoom in far enough on any smooth surface, it looks like a flat **plane**. This is the **tangent plane**.\n\n> **Definition: Local Linear Approximation**\n> The local linear approximation of a function $f(x,y)$ at a point $(a,b)$ is given by:\n> $$ L(x, y) = f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) $$\n> The graph of this function, $z = L(x,y)$, is the **tangent plane** to the surface $z = f(x,y)$ at the point $(a,b)$.\n\nThis formula is a beautiful extension of the 1D case. It says the approximate height near $(a,b)$ is the starting height $f(a,b)$, plus the change due to moving in x (slope in x times distance in x), plus the change due to moving in y (slope in y times distance in y).\n\n### Problem and Application: Estimating Values\n\nLet's find the tangent plane for the function $f(x,y) = \\sqrt{x^2 + y^2}$ (a cone) at the point $(3, 4)$ and use it to approximate $f(3.01, 3.99)$.\n\n```{python}\n#| label: linear-approx-problem\n#| fig-cap: \"Using SymPy to build the linear approximation L(x,y).\"\nimport sympy as sp\n\n# Define symbols and the function\nx, y = sp.symbols('x y')\nf = sp.sqrt(x**2 + y**2)\na, b = 3, 4\n\n# 1. Find the value of the function at (a,b)\nf_val = f.subs([(x, a), (y, b)])\nprint(f\"The value f({a},{b}) is: {f_val}\")\n\n# 2. Find the partial derivatives\nfx = sp.diff(f, x)\nfy = sp.diff(f, y)\nprint(f\"∂f/∂x = {fx}\")\nprint(f\"∂f/∂y = {fy}\")\n\n# 3. Find the slope values at (a,b)\nfx_val = fx.subs([(x, a), (y, b)])\nfy_val = fy.subs([(x, a), (y, b)])\nprint(f\"\\nThe slope fx({a},{b}) is: {fx_val}\")\nprint(f\"The slope fy({a},{b}) is: {fy_val}\")\n\n# 4. Assemble the linear approximation L(x,y)\nL = f_val + fx_val * (x - a) + fy_val * (y - b)\nprint(f\"\\nThe Tangent Plane equation is: z = {sp.simplify(L)}\")\n\n# 5. Use L to approximate f(3.01, 3.99)\napprox_val = L.subs([(x, 3.01), (y, 3.99)])\nprint(f\"\\nThe approximate value of f(3.01, 3.99) is: {approx_val}\")\n\n# 6. Compare with the true value\ntrue_val = f.subs([(x, 3.01), (y, 3.99)])\nprint(f\"The true value is: {true_val.evalf()}\")\nprint(f\"The approximation is excellent!\")\n\n```\n\n### Visualization: Surface and Tangent Plane\n\nSeeing is believing. Let's plot the cone and its tangent plane at $(3, 4, 5)$. Notice how the plane perfectly \"kisses\" the surface at that single point.\n\n```{python}\n#| label: fig-tangent-plane\n#| fig-cap: \"The tangent plane (red) provides a linear approximation to the surface (blue) at the point of tangency.\"\n\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Define the surface function\ndef f_np(x, y):\n    return np.sqrt(x**2 + y**2)\n\n# Create grid for the surface plot\nx_surf = np.linspace(0, 6, 50)\ny_surf = np.linspace(0, 8, 50)\nX_surf, Y_surf = np.meshgrid(x_surf, y_surf)\nZ_surf = f_np(X_surf, Y_surf)\n\n# Tangent plane: L(x,y) = 5 + (3/5)(x-3) + (4/5)(y-4)\ndef L_np(x, y):\n    return 5 + (3/5)*(x - 3) + (4/5)*(y - 4)\n\nX_plane, Y_plane = np.meshgrid(np.linspace(1, 5, 10), np.linspace(2, 6, 10))\nZ_plane = L_np(X_plane, Y_plane)\n\n# Create the plot\nfig = go.Figure()\n\n# Add the surface\nfig.add_trace(go.Surface(z=Z_surf, x=X_surf, y=Y_surf, opacity=0.8, name='f(x,y)'))\n\n# Add the tangent plane\nfig.add_trace(go.Surface(z=Z_plane, x=X_plane, y=Y_plane,\n                         colorscale='Reds', showscale=False, name='Tangent Plane'))\n\n# Add the point of tangency (3,4,5)\nfig.add_trace(go.Scatter3d(\n    x=[3], y=[4], z=[5],\n    mode='markers',\n    marker=dict(size=8, color='black'),\n    name='Point (3,4,5)'\n))\n\nfig.update_layout(title='Surface and its Tangent Plane',\n                  width=800, height=600, autosize=False)\nfig.show()\n\n```\n\n\n## The Chain Rule: Derivatives on a Path\n\nWhat if you're not standing still, but walking along a path on the map? Suppose your path is given by $(x(t), y(t))$. Your altitude is then $z = f(x(t), y(t))$. How fast is your altitude changing with respect to time, $t$?\n\nThe **multivariable chain rule** gives the answer:\n$$ \\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt} $$\n\nThis formula has a beautiful, compact form using the gradient and the velocity vector of your path, $r'(t) = \\begin{bmatrix} dx/dt \\\\ dy/dt \\end{bmatrix}$:\n$$ \\frac{dz}{dt} = \\nabla f \\cdot r'(t) $$\nThe rate of change of your altitude is the dot product of the \"steepest uphill\" vector and your direction of travel vector. This is a perfect example of how linear algebra and calculus work together.\n\n## Tutorial 6: Applications of Chain Rule\n\n### Problem 1: Voltage Variation in a Temperature-Dependent Circuit\n\nA resistive circuit has voltage $V = I \\cdot R$, where current and resistance vary with temperature $T$:\n\n$$\nI(T) = 5\\sqrt{T}, \\quad R(T) = 2T + 3\n$$ \n\nUsing the chain rule, find $\\dfrac{dV}{dT}$ and interpret how voltage changes as temperature increases. Verify by substitution.\n\n**Chain-rule solution:**\n\n1. Intermediate variable: $V(I(T),R(T))$  \n2. Partial derivatives:\n\n$$\n\\frac{\\partial V}{\\partial I} = R, \\quad \\frac{\\partial V}{\\partial R} = I\n$$  \n\n3. Derivatives of intermediate variables:\n\n$$\n\\frac{dI}{dT} = \\frac{5}{2} T^{-1/2}, \\quad \\frac{dR}{dT} = 2\n$$  \n\n4. Apply chain rule:\n\n$$\n\\frac{dV}{dT} = \\frac{\\partial V}{\\partial I}\\frac{dI}{dT} + \\frac{\\partial V}{\\partial R}\\frac{dR}{dT} = R \\cdot \\frac{5}{2}T^{-1/2} + I \\cdot 2\n$$  \n\n5. Substitute $I(T)$ and $R(T)$:\n\n$$\n\\frac{dV}{dT} = (2T+3) \\cdot \\frac{5}{2}T^{-1/2} + 5\\sqrt{T} \\cdot 2 = 15\\sqrt{T} +\\frac{15}{2\\sqrt{T}}\n$$\n\n**Verification by substitution:**\n\n$$\nV(T) = 5\\sqrt{T}(2T+3) \\implies \\frac{dV}{dT} = 15T^{1/2} + \\frac{15}{2T^{1/2}}\n$$\n\n**Python `SymPy` code:**\n```{python}\nimport sympy as sp\n\nT = sp.symbols('T', positive=True)\nI = 5*sp.sqrt(T)\nR = 2*T + 3\nV = I*R\n\n# Direct derivative w.r.t T\ndV_dT = sp.diff(V, T)\ndV_dT\n```\n\n>*Alternate method:*\n\n```{python}\nT = sp.symbols('T', positive=True)\nI, R = sp.symbols('I R', real=True)  # treat as independent\nV = I*R\n\n# Partial derivatives\ndV_dI = sp.diff(V, I)   # R\ndV_dR = sp.diff(V, R)   # I\n\n# Substitute expressions for I and R\nI_expr = 5*sp.sqrt(T)\nR_expr = 2*T + 3\n\n# Derivatives of I and R w.r.t T\ndI_dT = sp.diff(I_expr, T)\ndR_dT = sp.diff(R_expr, T)\n\n# Chain rule\ndV_dT = dV_dI.subs(I,I_expr).subs(R,R_expr)*dI_dT + dV_dR.subs(I,I_expr)*dR_dT\nsp.simplify(dV_dT)\n```\n\n### Problem 2: Power Output in a Time-Varying Transistor\n\nIn a transistor circuit, the instantaneous power is given by $P = V^2 / R$, where $V$ is the voltage across the transistor and $R$ is the resistance of the load. Suppose the voltage varies with time due to a decaying input signal, and the resistance slowly changes due to heating effects:\n\n$$\nV(t) = 10 e^{-0.02 t} \\text{ volts}, \\quad R(t) = 4 + 0.1 t \\ \\Omega\n$$\n\nCompute the **rate of change of power** $\\dfrac{dP}{dt}$ at any time $t$ using the **multivariable chain rule**, and verify the result by direct differentiation.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $P$ as a function of two variables: $P(V,R)$.\n\n$$\nP(V,R) = \\frac{V^2}{R}\n$$\n\n2. Compute the **partial derivatives**:\n\n$$\n\\frac{\\partial P}{\\partial V} = \\frac{2V}{R}, \\quad\n\\frac{\\partial P}{\\partial R} = -\\frac{V^2}{R^2}\n$$\n\n3. Compute **derivatives of intermediate variables** w.r.t. $t$:\n\n$$\n\\frac{dV}{dt} = \\frac{d}{dt}(10 e^{-0.02 t}) = -0.2 e^{-0.02 t}, \\quad\n\\frac{dR}{dt} = \\frac{d}{dt}(4 + 0.1 t) = 0.1\n$$\n\n4. Apply the **multivariable chain rule**:\n\n$$\n\\frac{dP}{dt} = \\frac{\\partial P}{\\partial V} \\frac{dV}{dt} + \\frac{\\partial P}{\\partial R} \\frac{dR}{dt}\n$$\n\nSubstitute the values:\n\n$$\n\\frac{dP}{dt} = \\frac{2V}{R} \\cdot (-0.2 e^{-0.02 t}) + \\left(-\\frac{V^2}{R^2}\\right) \\cdot 0.1\n$$\n\n5. Substitute $V(t)$ and $R(t)$:\n\n$$\n\\frac{dP}{dt} = \\frac{2 \\cdot 10 e^{-0.02 t}}{4 + 0.1 t} \\cdot (-0.2 e^{-0.02 t}) - \\frac{(10 e^{-0.02 t})^2}{(4 + 0.1 t)^2} \\cdot 0.1\n$$\n\nSimplify:\n\n$$\n\\frac{dP}{dt} = - \\frac{4 (e^{-0.04 t})}{4 + 0.1 t} - \\frac{100 e^{-0.04 t}}{(4 + 0.1 t)^2}\n$$\n\nThis is the **rate of change of power** at any time $t$.\n\n\n**Verification by direct differentiation:**\n\nDirectly compute:\n\n$$\nP(t) = \\frac{(10 e^{-0.02 t})^2}{4 + 0.1 t} = \\frac{100 e^{-0.04 t}}{4 + 0.1 t}\n$$\n\nDifferentiating w.r.t $t$ gives exactly the same expression:\n\n$$\n\\frac{dP}{dt} = - \\frac{4 e^{-0.04 t}}{4 + 0.1 t} - \\frac{100 e^{-0.04 t}}{(4 + 0.1 t)^2}\n$$\n\n>**Python code:**\n\n```{python}\nimport sympy as sp\n\nt = sp.symbols('t', real=True)\n# Define independent symbols for chain rule\nV_sym, R_sym = sp.symbols('V R', real=True)\nP = V_sym**2 / R_sym\n\n# Partial derivatives\ndP_dV = sp.diff(P, V_sym)   # 2*V/R\ndP_dR = sp.diff(P, R_sym)   # -V^2 / R^2\n\n# Expressions for V(t) and R(t)\nV_expr = 10*sp.exp(-0.02*t)\nR_expr = 4 + 0.1*t\n\n# Derivatives of intermediate variables\ndV_dt = sp.diff(V_expr, t)\ndR_dt = sp.diff(R_expr, t)\n\n# Chain rule\ndP_dt = dP_dV.subs({V_sym:V_expr, R_sym:R_expr})*dV_dt + dP_dR.subs({V_sym:V_expr, R_sym:R_expr})*dR_dt\ndP_dt_simplified = sp.simplify(dP_dt)\ndP_dt_simplified\n```\n### Problem 3:  Capacitance Sensitivity in a Temperature-Dependent Capacitor\n\nA parallel-plate capacitor has capacitance \n\n$$\nC = \\varepsilon \\frac{A}{d},\n$$ \n\nwhere $A$ is the plate area, $d$ is the separation, and $\\varepsilon$ is the permittivity. The capacitor is subject to temperature variations that change the radius of the circular plates and the separation:\n\n$$\nA = \\pi r^2, \\quad r = 2 + 0.01 T, \\quad d = 1 + 0.005 T\n$$\n\nCompute the *rate of change of capacitance* $\\dfrac{dC}{dT}$ using the **multivariable chain rule** and verify by direct differentiation.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $C$ as a function of two variables: $C(A(T), d(T))$.\n\n$$\nC(A,d) = \\frac{\\varepsilon A}{d}\n$$\n\n2. Compute the **partial derivatives**:\n\n$$\n\\frac{\\partial C}{\\partial A} = \\frac{\\varepsilon}{d}, \\quad\n\\frac{\\partial C}{\\partial d} = -\\frac{\\varepsilon A}{d^2}\n$$\n\n3. Compute **derivatives of intermediate variables** w.r.t $T$:\n\n$$\n\\frac{dA}{dT} = \\frac{d}{dT} (\\pi r^2) = 2\\pi r \\frac{dr}{dT} = 2 \\pi (2+0.01T)(0.01) = 0.02 \\pi (2+0.01T)\n$$\n\n$$\n\\frac{dd}{dT} = \\frac{d}{dT}(1 + 0.005 T) = 0.005\n$$\n\n4. Apply the **multivariable chain rule**:\n\n$$\n\\frac{dC}{dT} = \\frac{\\partial C}{\\partial A} \\frac{dA}{dT} + \\frac{\\partial C}{\\partial d} \\frac{dd}{dT}\n$$\n\nSubstitute the partial derivatives:\n\n$$\n\\frac{dC}{dT} = \\frac{\\varepsilon}{d} \\cdot 0.02 \\pi (2+0.01T) - \\frac{\\varepsilon A}{d^2} \\cdot 0.005\n$$\n\n5. Substitute $A = \\pi r^2 = \\pi (2 + 0.01 T)^2$ and $d = 1 + 0.005 T$:\n\n$$\n\\frac{dC}{dT} = \\frac{\\varepsilon \\cdot 0.02 \\pi (2+0.01T)}{1+0.005T} - \\frac{\\varepsilon \\pi (2+0.01T)^2 \\cdot 0.005}{(1+0.005T)^2}\n$$\n\nThis gives the **rate of change of capacitance** at any temperature $T$.\n\n**Verification by direct differentiation:**\n\nDirectly differentiate:\n\n$$\nC(T) = \\frac{\\varepsilon \\pi (2+0.01 T)^2}{1 + 0.005 T}\n$$\n\nw.r.t $T$ to get exactly the same expression as above.\n\n**Python SymPy Code (Corrected Chain-Rule Implementation):**\n\n```{python}\nimport sympy as sp\n\nT = sp.symbols('T', real=True)\neps = sp.symbols('eps', real=True)\n\n# Define intermediate variables as symbols for chain rule\nA_sym, d_sym = sp.symbols('A d', real=True)\nC = eps * A_sym / d_sym\n\n# Partial derivatives\ndC_dA = sp.diff(C, A_sym)   # eps / d\ndC_dd = sp.diff(C, d_sym)   # -eps*A / d^2\n\n# Expressions for A(T) and d(T)\nr_expr = 2 + 0.01*T\nA_expr = sp.pi * r_expr**2\nd_expr = 1 + 0.005*T\n\n# Derivatives of intermediate variables\ndA_dT = sp.diff(A_expr, T)\ndd_dT = sp.diff(d_expr, T)\n\n# Chain rule\ndC_dT = dC_dA.subs({A_sym:A_expr, d_sym:d_expr})*dA_dT + dC_dd.subs({A_sym:A_expr, d_sym:d_expr})*dd_dT\ndC_dT_simplified = sp.simplify(dC_dT)\ndC_dT_simplified\n```\n\n### Problem 4: Neural Network Weight Sensitivity\n\nConsider a simple neuron in a feedforward neural network with two inputs $x_1$ and $x_2$, weights $w_1$ and $w_2$, and bias $b$. The output of the neuron is  \n\n$$\nz = \\tanh(u), \\quad u = w_1 x_1 + w_2 x_2 + b\n$$  \n\nCompute the *sensitivity of the output* with respect to the weights, i.e., $\\frac{\\partial z}{\\partial w_1}$ and $\\frac{\\partial z}{\\partial w_2}$, using the *multivariable chain rule*. Verify the results using Python's SymPy library.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $z$ as a function of $u$: $z = \\tanh(u)$.  \n2. Compute the derivative of $z$ with respect to $u$:\n\n$$\n\\frac{dz}{du} = 1 - \\tanh^2(u)\n$$\n\n3. Compute the derivatives of $u$ with respect to the weights:\n\n$$\n\\frac{\\partial u}{\\partial w_1} = x_1, \\quad \\frac{\\partial u}{\\partial w_2} = x_2\n$$\n\n4. Apply the **chain rule**:\n\n$$\n\\frac{\\partial z}{\\partial w_1} = \\frac{dz}{du} \\cdot \\frac{\\partial u}{\\partial w_1} = (1 - \\tanh^2(u)) \\cdot x_1\n$$\n\n$$\n\\frac{\\partial z}{\\partial w_2} = \\frac{dz}{du} \\cdot \\frac{\\partial u}{\\partial w_2} = (1 - \\tanh^2(u)) \\cdot x_2\n$$\n\n\n**Verification using Python SymPy:**\n\n```{python}\nimport sympy as sp\n\n# Define symbols\nw1, w2, x1, x2, b = sp.symbols('w1 w2 x1 x2 b', real=True)\n\n# Define u and z\nu = w1*x1 + w2*x2 + b\nz = sp.tanh(u)\n\n# Compute derivatives using chain rule automatically\ndz_dw1 = sp.diff(z, w1)\ndz_dw2 = sp.diff(z, w2)\n\n# Simplify\ndz_dw1_simpl = sp.simplify(dz_dw1)\ndz_dw2_simpl = sp.simplify(dz_dw2)\n\ndz_dw1_simpl, dz_dw2_simpl\n```\n\n### Problem 5: Temperature in a Polar Sensor Grid\n \nA sensor grid measures temperature at points $(x, y)$, where the temperature depends on position as  \n\n$$\nT = x^2 + y^2\n$$  \n\nSuppose the sensors are arranged in a polar coordinate system:\n\n$$\nx = r \\cos\\theta, \\quad y = r \\sin\\theta\n$$  \n\nCompute the *rate of change of temperature* with respect to the radial distance $r$ and the angular position $\\theta$, i.e., $\\frac{\\partial T}{\\partial r}$ and $\\frac{\\partial T}{\\partial \\theta}$, using the *multivariable chain rule*. Verify the results using Python `SymPy`.\n\n\n**Solution (Chain-Rule Method):**\n\n1. Treat $T$ as a function of $x$ and $y$:\n\n$$\nT(x,y) = x^2 + y^2\n$$\n\nPartial derivatives:\n\n$$\n\\frac{\\partial T}{\\partial x} = 2x, \\quad \\frac{\\partial T}{\\partial y} = 2y\n$$\n\n2. Compute derivatives of $x$ and $y$ with respect to $r$ and $\\theta$:\n\n$$\n\\frac{\\partial x}{\\partial r} = \\cos\\theta, \\quad \\frac{\\partial x}{\\partial \\theta} = -r \\sin\\theta\n$$\n\n$$\n\\frac{\\partial y}{\\partial r} = \\sin\\theta, \\quad \\frac{\\partial y}{\\partial \\theta} = r \\cos\\theta\n$$\n\n3. Apply the **multivariable chain rule**:\n\n$$\n\\frac{\\partial T}{\\partial r} = \\frac{\\partial T}{\\partial x} \\frac{\\partial x}{\\partial r} + \\frac{\\partial T}{\\partial y} \\frac{\\partial y}{\\partial r} = 2x \\cos\\theta + 2y \\sin\\theta\n$$\n\n$$\n\\frac{\\partial T}{\\partial \\theta} = \\frac{\\partial T}{\\partial x} \\frac{\\partial x}{\\partial \\theta} + \\frac{\\partial T}{\\partial y} \\frac{\\partial y}{\\partial \\theta} = 2x(-r \\sin\\theta) + 2y(r \\cos\\theta) = 0\n$$\n\n4. Substitute $x = r \\cos\\theta$, $y = r \\sin\\theta$:\n\n$$\n\\frac{\\partial T}{\\partial r} = 2r (\\cos^2\\theta + \\sin^2\\theta) = 2r\n$$\n\n$$\n\\frac{\\partial T}{\\partial \\theta} = 0\n$$\n\n\n**Verification using Python SymPy:**\n\n```{python}\nimport sympy as sp\n\n# Define symbols\nr, theta = sp.symbols('r theta', real=True)\n\n# Define coordinate transformations\nx = r * sp.cos(theta)\ny = r * sp.sin(theta)\n\n# Temperature function\nT = x**2 + y**2\n\n# Partial derivatives w.r.t r and theta\ndT_dr = sp.diff(T, r)\ndT_dtheta = sp.diff(T, theta)\n\ndT_dr_simpl = sp.simplify(dT_dr)\ndT_dtheta_simpl = sp.simplify(dT_dtheta)\n\ndT_dr_simpl, dT_dtheta_simpl\n```\n\n### Problem 6: Dynamic System Response Over Time\n\nA dynamic system has an output that depends multiplicatively on two time-varying inputs. The instantaneous output is\n\n$$\nz = e^{\\,x y},\n$$\n\nwhere the inputs themselves vary with time as\n\n$$\nx(t) = 2t^2,\\qquad y(t) = 3t + 1.\n$$\n\nUse the *multivariable chain rule* to compute the total derivative $\\dfrac{dz}{dt}$ (i.e. the rate of change of the output with respect to time). Then *verify* the result by substituting $x(t),y(t)$ into $z$ and differentiating directly. Evaluate the rate at $t=1$ and interpret the result.\n\n**Solution (Chain-Rule Method):**\n\n1. Identify intermediate variables: $z = f(x,y)$ with $f(x,y)=e^{xy}$, and $x=x(t),\\ y=y(t)$.\n\n2. Compute the partial derivatives of $z$ w.r.t the intermediate variables:\n\n$$\n\\frac{\\partial z}{\\partial x} = \\frac{\\partial}{\\partial x} e^{xy} = y\\,e^{xy}, \\qquad\n\\frac{\\partial z}{\\partial y} = \\frac{\\partial}{\\partial y} e^{xy} = x\\,e^{xy}.\n$$\n\n3. Compute time-derivatives of the intermediate variables:\n\n$$\n\\frac{dx}{dt} = \\frac{d}{dt}(2t^2) = 4t, \\qquad\n\\frac{dy}{dt} = \\frac{d}{dt}(3t+1) = 3.\n$$\n\n4. Apply the multivariable chain rule (total derivative):\n\n$$\n\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial z}{\\partial y}\\frac{dy}{dt}.\n$$\n\nSubstitute the partials and time-derivatives:\n\n$$\n\\frac{dz}{dt} = \\bigl(y e^{xy}\\bigr)(4t) + \\bigl(x e^{xy}\\bigr)(3)\n= e^{xy}\\bigl(4t y + 3x\\bigr).\n$$\n\n5. Evaluate at $t=1$. First compute $x(1)=2\\cdot1^2 = 2$ and $y(1)=3\\cdot1+1 = 4$. Thus\n\n$$\n\\frac{dz}{dt}\\Big|_{t=1} = e^{(2)(4)}\\bigl(4\\cdot1\\cdot4 + 3\\cdot2\\bigr) = e^{8}(16 + 6) = 22 e^{8}.\n$$\n\n**Interpretation:** At $t=1$ the output is increasing very rapidly: the instantaneous rate is $22e^8$, reflecting strong sensitivity because $z$ is exponential in the product $xy$.\n\n**Verification by substitution (direct differentiation):**\n\nSubstitute $x(t)$ and $y(t)$ into $z$:\n\n$$\nz(t) = e^{(2t^2)(3t+1)} = e^{6t^3 + 2t^2}.\n$$\n\nDifferentiate directly:\n\n$$\n\\frac{dz}{dt} = e^{6t^3 + 2t^2}\\cdot(18t^2 + 4t).\n$$\n\nCheck algebraically that\n\n$$\n18t^2 + 4t \\equiv 4t(3t+1) + 3(2t^2) = 4t y + 3x,\n$$\n\nso the direct differentiation result matches the chain-rule result. Evaluating at $t=1$ gives $22e^8$ as before.\n\n**Python (SymPy) verification code:**\n\n```{python}\nimport sympy as sp\n\n# symbol\nt = sp.symbols('t', real=True)\n\n# define x(t), y(t)\nx_expr = 2*t**2\ny_expr = 3*t + 1\n\n# Method A: explicit chain-rule via partials (use x,y as symbols then substitute)\nx_sym, y_sym = sp.symbols('x_sym y_sym')\nz_sym = sp.exp(x_sym*y_sym)\n\n# partials\nz_x = sp.diff(z_sym, x_sym)   # y * exp(xy)\nz_y = sp.diff(z_sym, y_sym)   # x * exp(xy)\n\n# substitute x(t), y(t) and multiply by dx/dt, dy/dt\ndzdt_chain = (z_x.subs({x_sym: x_expr, y_sym: y_expr}) * sp.diff(x_expr, t) +\n              z_y.subs({x_sym: x_expr, y_sym: y_expr}) * sp.diff(y_expr, t))\ndzdt_chain_simpl = sp.simplify(dzdt_chain)\n\n# evaluate at t=1\ndzdt_chain_at_1 = dzdt_chain_simpl.subs(t, 1)\n\n# Method B: substitute then differentiate\nz_sub = sp.exp(x_expr * y_expr)\ndzdt_sub = sp.diff(z_sub, t)\ndzdt_sub_simpl = sp.simplify(dzdt_sub)\ndzdt_sub_at_1 = dzdt_sub_simpl.subs(t, 1)\n\ndzdt_chain_simpl, dzdt_chain_at_1, dzdt_sub_simpl, dzdt_sub_at_1\n```\n## The Main Event: Finding Maxima and Minima\n\nNow we can answer the big question: how do we find the peaks and valleys of our landscape?\n\nAt the very top of a peak or the bottom of a valley, the ground is perfectly flat. The slope in *every* direction is zero. This means both partial derivatives must be zero.\n\n> **Critical Points**\n> A point $(a, b)$ is a **critical point** of $f(x,y)$ if the gradient at that point is the zero vector:\n> $$ \\nabla f(a, b) = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\quad \\text{which means} \\quad f_x(a, b) = 0 \\text{ and } f_y(a, b) = 0 $$\n\nBut a flat spot isn't always a peak or a valley. It could also be a **saddle point**, like the middle of a Pringles chip—it's a minimum in one direction and a maximum in another.\n\nTo classify these critical points, we need a multivariable version of the Second Derivative Test. This test involves a quantity called the **Discriminant** ($D$), which is built from the second-order partial derivatives.\n\n::: {.callout-note}\n### The Second Derivative Test\n\nFirst, find all critical points by solving $\\nabla f = 0$. Then, for each critical point $(a, b)$, calculate the second partial derivatives ($f_{xx}, f_{yy}, f_{xy}$) at that point.\n\nDefine the Discriminant $D = f_{xx}(a,b) f_{yy}(a,b) - [f_{xy}(a,b)]^2$.\n\n1.  If $D > 0$ and $f_{xx}(a,b) > 0$, then $f$ has a **local minimum** at $(a, b)$.\n2.  If $D > 0$ and $f_{xx}(a,b) < 0$, then $f$ has a **local maximum** at $(a, b)$.\n3.  If $D < 0$, then $f$ has a **saddle point** at $(a, b)$.\n4.  If $D = 0$, the test is inconclusive.\n:::\n\nNote that the Discriminant is just the determinant of the **Hessian matrix**, a beautiful connection back to linear algebra!\n$$ H = \\begin{bmatrix} f_{xx} & f_{xy} \\\\ f_{yx} & f_{yy} \\end{bmatrix} \\implies D = \\det(H) $$\n\n### Example: Finding the Extrema of Our Landscape\n\nLet's use `SymPy` to find and classify all the critical points of the function $f(x, y) = (x^2 + 3y^2) e^{1 - x^2 - y^2}$ we plotted at the beginning.\n\n```{python}\n#| label: find-classify-extrema\n#| fig-cap: \"Using SymPy to find and classify the critical points of our surface.\"\n\nimport sympy as sp\n\n# Define symbols and the function\nx, y = sp.symbols('x y')\nf = (x**2 + 3*y**2) * sp.exp(1 - x**2 - y**2)\n\n# 1. Find the partial derivatives\nfx = sp.diff(f, x)\nfy = sp.diff(f, y)\n\n# 2. Find the critical points by solving ∇f = 0\n# This can be computationally intensive; we'll use a numerical approach for clarity\n# For this specific function, inspection shows critical points at:\n# (0,0), (1,0), (-1,0), (0,1), (0,-1)\ncritical_points = [\n    (0, 0),\n    (1, 0),\n    (-1, 0),\n    (0, 1),\n    (0, -1)\n]\nprint(f\"The critical points are: {critical_points}\\n\")\n\n\n# 3. Calculate second-order partial derivatives\nfxx = sp.diff(fx, x)\nfyy = sp.diff(fy, y)\nfxy = sp.diff(fx, y)\n\n# 4. Create the Discriminant D\nD = fxx * fyy - fxy**2\n\n# 5. Classify each critical point\nfor p in critical_points:\n    px, py = p\n    # Substitute the point's coordinates into D and fxx\n    D_val = D.subs([(x, px), (y, py)])\n    fxx_val = fxx.subs([(x, px), (y, py)])\n    \n    print(f\"--- Analyzing point {p} ---\")\n    print(f\"  D = {D_val:.2f}, f_xx = {fxx_val:.2f}\")\n\n    if D_val > 0 and fxx_val > 0:\n        print(\"  Result: Local Minimum\")\n    elif D_val > 0 and fxx_val < 0:\n        print(\"  Result: Local Maximum\")\n    elif D_val < 0:\n        print(\"  Result: Saddle Point\")\n    else:\n        print(\"  Result: Test is inconclusive\")\n```\n\nThe results match what we see in the 3D plot perfectly! The origin is a local minimum, the points on the y-axis are local maxima (the two highest peaks), and the points on the x-axis are saddle points.\n\n## Module III Summary\n\n*   We've moved from 2D curves to 3D surfaces, or \"landscapes.\"\n*   **Partial derivatives** ($f_x, f_y$) are the slopes in the cardinal directions.\n*   The **gradient vector** ($\\nabla f = [f_x, f_y]$) packages these slopes and points in the direction of steepest ascent. It is the key to understanding the local geometry of a surface.\n*   To find potential maxima and minima (**critical points**), we find where the landscape is flat by solving $\\nabla f = 0$.\n*   The **Second Derivative Test**, using the determinant of the Hessian matrix, allows us to classify these critical points as local maxima, local minima, or saddle points.\n*   This process of finding extrema is called **optimization**, and it is the absolute core of how modern AI models are trained.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"module3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.31","bibliography":["references.bib"],"jupyter":"python3","theme":"cosmo","fig-cap-location":"bottom","mathjax":true,"title":"Module 3: Multivariable Calculus - Differentiation"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","include-in-header":[{"text":"\\usepackage{amsmath}\n\\usepackage{amssymb}\n"}],"output-file":"module3.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"jupyter":"python3","title":"Module 3: Multivariable Calculus - Differentiation"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}