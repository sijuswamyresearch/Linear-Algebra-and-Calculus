[
  {
    "objectID": "module2.html",
    "href": "module2.html",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "",
    "text": "3.1 The Big Question: What Does a Matrix Do?\nIn the first module, we treated matrices as simple containers for the numbers in a linear system. But a matrix is much more than that. A matrix is a transformation. It takes a vector and maps it to a new vector.\nWhen you multiply a vector \\(x\\) by a matrix \\(A\\), the resulting vector \\(Ax\\) is usually stretched and rotated. It points in a new direction.\nBut for any given matrix, there are a few very special vectors. When you multiply these special vectors by the matrix, they do not change direction. They only get stretched or shrunk. The output vector \\(Ax\\) is parallel to the input vector \\(x\\).\nThese special vectors are the eigenvectors of the matrix, and the scaling factor is the eigenvalue.\nThe eigenvectors tell you the “axes” of the transformation, the directions that are preserved. The eigenvalues tell you the scaling factor along those axes. If you know the eigenvalues and eigenvectors of a matrix, you understand its fundamental behavior.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#the-big-question-what-does-a-matrix-do",
    "href": "module2.html#the-big-question-what-does-a-matrix-do",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "",
    "text": "Definition: Eigenvalue and Eigenvector For a square matrix \\(A\\), a non-zero vector \\(x\\) is an eigenvector if it satisfies the equation: \\[ Ax = \\lambda x \\] where \\(\\lambda\\) is a scalar known as the eigenvalue corresponding to the eigenvector \\(x\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#finding-eigenvalues-and-eigenvectors",
    "href": "module2.html#finding-eigenvalues-and-eigenvectors",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.2 Finding Eigenvalues and Eigenvectors",
    "text": "3.2 Finding Eigenvalues and Eigenvectors\nHow do we find these special numbers \\(\\lambda\\) and vectors \\(x\\)? We start by rewriting the main equation.\n\\[\n\\begin{align*}\nAx &= \\lambda x \\\\\nAx - \\lambda x &= 0 \\\\\nAx - \\lambda I x &= 0 && \\text{(where I is the identity matrix)} \\\\\n(A - \\lambda I)x &= 0\n\\end{align*}\n\\]\nLook at that last line! It’s a homogeneous system of equations, just like we saw in Module I. We are looking for a non-zero solution for \\(x\\). For the system \\((A - \\lambda I)x = 0\\) to have a non-trivial solution, the matrix \\((A - \\lambda I)\\) must be singular.\nAnd what does it mean for a matrix to be singular? Its determinant must be zero.\n\nThe Characteristic Equation \\[ \\det(A - \\lambda I) = 0 \\]\n\nSolving this equation for \\(\\lambda\\) gives us the eigenvalues. Then, for each eigenvalue, we plug it back into \\((A - \\lambda I)x = 0\\) to find the corresponding eigenvector(s).\n\n\n\n\n\n\nShortcut for finding eigen values of 2x2 Matrices\n\n\n\nFor any 2x2 matrix \\(A = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\), the characteristic equation is always: \\[ \\lambda^2 - (\\text{trace}(A))\\lambda + \\det(A) = 0 \\] Where:\n\nThe trace is the sum of the diagonal elements: \\(\\text{trace}(A) = a+d\\).\nThe determinant is \\(\\det(A) = ad-bc\\).\n\nThis is a beautiful and powerful result! You don’t need to calculate \\(\\det(A - \\lambda I)\\) from scratch; just find the trace and determinant and plug them into the quadratic formula.\n\n\n\n\n\n\n\n\nShortcut for finding eigen values of 3x3 Matrices\n\n\n\nA similar, though more complex, shortcut exists for 3x3 matrices. The characteristic equation is: \\[ \\lambda^3 - (\\text{trace}(A))\\lambda^2 + C\\lambda - \\det(A) = 0 \\] Where \\(C\\) is the sum of the determinants of the 2x2 principal minors (the matrices you get by deleting the \\(i\\)-th row and \\(i\\)-th column). \\[ C = \\det\\begin{pmatrix} a_{22} & a_{23} \\\\ a_{32} & a_{33} \\end{pmatrix} + \\det\\begin{pmatrix} a_{11} & a_{13} \\\\ a_{31} & a_{33} \\end{pmatrix} + \\det\\begin{pmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{pmatrix} \\] While this formula works, solving a cubic equation can be difficult, and this is where numerical methods often become more practical.\n\n\n\n3.2.1 Example: A Symbolic Approach\nLet’s do this for a simple matrix \\(A = \\begin{bmatrix} 4 & -2 \\\\ 1 & 1 \\end{bmatrix}\\) using SymPy to see all the steps.\n\n\nCode\nimport sympy as sp\n\n# Define our matrix A\nA = sp.Matrix([\n    [4, -2],\n    [1,  1]\n])\n\n# Create a symbol for lambda\nlam = sp.symbols('lambda')\n\n# Create the Identity matrix of the same size as A\nI = sp.eye(A.shape[0])  # Corrected\n\n# Form the matrix (A - lambda*I)\nchar_matrix = A - lam * I\n\nprint(\"The matrix (A - λI):\")\nsp.pprint(char_matrix)\n\n# Calculate the determinant to get the characteristic polynomial\nchar_poly = char_matrix.det()\nprint(f\"\\nThe characteristic polynomial det(A - λI) is: {char_poly}\")\n\n# Solve the characteristic equation det(A - λI) = 0 for the eigenvalues\neigenvalues = sp.solve(char_poly, lam)\nprint(f\"\\nThe eigenvalues are: {eigenvalues}\")\n\n# SymPy can also do this in one step\nprint(\"\\n--- Using SymPy's built-in function ---\")\nsp.pprint(A.eigenvects())\n\n\nThe matrix (A - λI):\n⎡4 - λ   -2  ⎤\n⎢            ⎥\n⎣  1    1 - λ⎦\n\nThe characteristic polynomial det(A - λI) is: lambda**2 - 5*lambda + 6\n\nThe eigenvalues are: [2, 3]\n\n--- Using SymPy's built-in function ---\n⎡⎛      ⎡⎡1⎤⎤⎞  ⎛      ⎡⎡2⎤⎤⎞⎤\n⎢⎜2, 1, ⎢⎢ ⎥⎥⎟, ⎜3, 1, ⎢⎢ ⎥⎥⎟⎥\n⎣⎝      ⎣⎣1⎦⎦⎠  ⎝      ⎣⎣1⎦⎦⎠⎦\n\n\nThe output shows that for our matrix \\(A\\), the eigenvalues are \\(\\lambda_1 = 3\\) and \\(\\lambda_2 = 2\\). The corresponding eigenvectors are multiples of \\(\\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\n\n\n3.2.2 Key Properties of Eigenvalues\nThe shortcuts above hint at a deeper connection. Here are the most important properties of eigenvalues for any \\(n \\times n\\) matrix \\(A\\).\n\nSum: The sum of the eigenvalues is equal to the trace of the matrix. \\[ \\sum_{i=1}^{n} \\lambda_i = \\text{trace}(A) \\]\nProduct: The product of the eigenvalues is equal to the determinant of the matrix. \\[ \\prod_{i=1}^{n} \\lambda_i = \\det(A) \\]\nSingularity: A matrix is singular (not invertible) if and only if at least one of its eigenvalues is zero. (This follows directly from the product property).\nPowers: The eigenvalues of \\(A^k\\) are \\(\\lambda_1^k, \\lambda_2^k, \\dots, \\lambda_n^k\\).\nInverse: The eigenvalues of \\(A^{-1}\\) are \\(1/\\lambda_1, 1/\\lambda_2, \\dots, 1/\\lambda_n\\).\nTranspose: A matrix and its transpose, \\(A\\) and \\(A^T\\), have the same eigenvalues.\nTriangular Matrices: The eigenvalues of a triangular (or diagonal) matrix are simply the entries on its main diagonal.\n\nLet’s verify a few of these properties with code.\n\n\nCode\nimport numpy as np\n\n# A sample 3x3 matrix\nA = np.array([\n    [2, 1, -1],\n    [6, -1, 0],\n    [-1, -2, -1]\n])\n\n# Get the eigenvalues using NumPy\neigenvalues = np.linalg.eigvals(A)\n\nprint(f\"The matrix A is:\\n{A}\")\nprint(f\"\\nIts eigenvalues are: {eigenvalues}\")\n\n# Verify the Sum property\nsum_of_eigenvalues = np.sum(eigenvalues)\ntrace_of_A = np.trace(A)\nprint(f\"\\nSum of eigenvalues: {sum_of_eigenvalues:.4f}\")\nprint(f\"Trace of A: {trace_of_A}\")\nprint(f\"Are they close? {np.isclose(sum_of_eigenvalues, trace_of_A)}\")\n\n# Verify the Product property\nproduct_of_eigenvalues = np.prod(eigenvalues)\ndeterminant_of_A = np.linalg.det(A)\nprint(f\"\\nProduct of eigenvalues: {product_of_eigenvalues:.4f}\")\nprint(f\"Determinant of A: {determinant_of_A:.4f}\")\nprint(f\"Are they close? {np.isclose(product_of_eigenvalues, determinant_of_A)}\")\n\n\nThe matrix A is:\n[[ 2  1 -1]\n [ 6 -1  0]\n [-1 -2 -1]]\n\nIts eigenvalues are: [ 3.91899444+0.j         -1.95949722+1.23243177j -1.95949722-1.23243177j]\n\nSum of eigenvalues: 0.0000+0.0000j\nTrace of A: 0\nAre they close? True\n\nProduct of eigenvalues: 21.0000+0.0000j\nDeterminant of A: 21.0000\nAre they close? True\n\n\n\n\n3.2.3 Example: Finding Eigenvectors by Hand\nNow that we have the eigenvalues, how do we find the eigenvectors? We solve the system \\((A - \\lambda I)x = 0\\). Let’s do this for our first example, \\(A = \\begin{bmatrix} 4 & -2 \\\\ 1 & 1 \\end{bmatrix}\\), where we found \\(\\lambda_1 = 3\\) and \\(\\lambda_2 = 2\\).\nCase 1: Find eigenvector for \\(\\lambda_1 = 3\\).\nWe need to solve \\((A - 3I)x = 0\\). \\[ (A - 3I) = \\begin{bmatrix} 4-3 & -2 \\\\ 1 & 1-3 \\end{bmatrix} = \\begin{bmatrix} 1 & -2 \\\\ 1 & -2 \\end{bmatrix} \\] So the system is: \\[ \\begin{bmatrix} 1 & -2 \\\\ 1 & -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\] Both rows give the same equation: \\(x_1 - 2x_2 = 0\\), or \\(x_1 = 2x_2\\). This system has a free variable! If we choose \\(x_2 = 1\\), then \\(x_1 = 2\\). So our eigenvector is a multiple of \\(v_1 = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}\\).\nCase 2: Find eigenvector for \\(\\lambda_2 = 2\\).\nWe need to solve \\((A - 2I)x = 0\\). \\[ (A - 2I) = \\begin{bmatrix} 4-2 & -2 \\\\ 1 & 1-2 \\end{bmatrix} = \\begin{bmatrix} 2 & -2 \\\\ 1 & -1 \\end{bmatrix} \\] The system is \\(2x_1 - 2x_2 = 0\\), or \\(x_1 = x_2\\). If we choose \\(x_2=1\\), then \\(x_1=1\\). So our eigenvector is a multiple of \\(v_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\).\nThese match the results we get from our computer programs perfectly!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#the-geometry-of-eigenvectors",
    "href": "module2.html#the-geometry-of-eigenvectors",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.3 The Geometry of Eigenvectors",
    "text": "3.3 The Geometry of Eigenvectors\nLet’s visualize what the matrix \\(A\\) does to the plane. We will take a circle of vectors, apply the matrix \\(A\\) to each one, and see where they land. The circle will be transformed into an ellipse.\nThe key thing to watch for is that the eigenvectors lie along the axes of this new ellipse. They are the only vectors that don’t get rotated off their original line.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the matrix A\nA = np.array([\n    [4, -2],\n    [1,  1]\n])\n\n# Get eigenvalues and eigenvectors\neigvals, eigvecs = np.linalg.eig(A)\nv1 = eigvecs[:, 0]\nv2 = eigvecs[:, 1]\n\n# Create a unit circle\ntheta = np.linspace(0, 2*np.pi, 200)\ncircle_vectors = np.array([np.cos(theta), np.sin(theta)])\n\n# Transform the circle into an ellipse\ntransformed_vectors = A @ circle_vectors\n\n# --- Plot ---\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Original circle (blue)\nax.plot(circle_vectors[0, :], circle_vectors[1, :], color='blue', label='Original Circle')\n\n# Transformed ellipse (green)\nax.plot(transformed_vectors[0, :], transformed_vectors[1, :], color='green', label='Transformed Ellipse')\n\n# Eigenvectors transformed (red dashed) — scaled by eigenvalues\nax.plot([0, eigvals[0] * v1[0]], [0, eigvals[0] * v1[1]], 'r--', linewidth=2, label=f'Eigenvector λ={eigvals[0]:.2f}')\nax.plot([0, eigvals[1] * v2[0]], [0, eigvals[1] * v2[1]], 'r--', linewidth=2, label=f'Eigenvector λ={eigvals[1]:.2f}')\n\n# Formatting\nax.set_aspect('equal', adjustable='box')\nax.set_xlim(-5, 5)\nax.set_ylim(-5, 5)\nax.set_xlabel('X-axis')\nax.set_ylabel('Y-axis')\nax.set_title('Geometric Action of a Matrix')\nax.legend()\nax.grid(True)\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.1: The unit circle (blue) is transformed into an ellipse (green). The eigenvectors (red) only stretch, keeping their direction.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#tutorial-3--eigen-values-and-eigen-vectors-in-engineering-applications",
    "href": "module2.html#tutorial-3--eigen-values-and-eigen-vectors-in-engineering-applications",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.4 Tutorial 3- Eigen values and Eigen vectors in Engineering Applications",
    "text": "3.4 Tutorial 3- Eigen values and Eigen vectors in Engineering Applications\nSolve each question using algebraic techniques: form the characteristic polynomial, compute eigenvalues, find eigenvectors (mode shapes/principal directions/steady-state vectors), and verify algebraic and geometric multiplicities.\n\n\n3.4.1 Questions\n\nMEMS Resonator — Vibration Analysis\n\nA MEMS resonator used in an oscillator circuit has stiffness matrix\n\\[K = \\begin{bmatrix}6 & -2 & 0 \\\\ -2 & 4 & -2 \\\\ 0 & -2 & 6\\end{bmatrix}.\\]\n\nFind the natural frequencies of vibration (eigenvalues of \\(K\\)).\nDetermine the mode shapes of vibration (eigenvectors of \\(K\\)).\nVerify algebraic and geometric multiplicities of each eigenfrequency.\nExplain the engineering meaning of each eigenvalue as a measure (e.g., squared natural frequency) and comment on stability.\n\n\n\nWireless Channel Reliability\n\nA discrete-time Markov model for a wireless channel has transition matrix\n\\[P = \\begin{bmatrix}0.8 & 0.2 \\\\ 0.3 & 0.7\\end{bmatrix}.\\]\n\nCompute the eigenvalues of \\(P\\).\nFind the steady-state reliability vector (the eigenvector corresponding to \\(\\lambda=1\\)) and normalize it to a probability vector.\nVerify algebraic and geometric multiplicities.\nInterpret the dominant and subdominant eigenvalues with respect to long-term reliability and convergence speed.\n\n\n\nImage Compression via PCA\n\nCovariance matrix of a 2-D feature (pixel intensities) is\n\\[C = \\begin{bmatrix}5 & 2 \\\\ 2 & 2\\end{bmatrix}.\\]\n\nFind the principal directions (eigenvectors) of \\(C\\).\nCompute the variance explained by each direction (eigenvalues).\nVerify algebraic and geometric multiplicities.\nCompute the percentage of total variance captured by the largest eigenvalue (compression efficiency).\n\n\n\nQuantum 2-Level System (Hamiltonian)\n\nThe Hamiltonian of a two-level quantum system is\n\\[H = \\begin{bmatrix}4 & 1 \\\\ 1 & 4\\end{bmatrix}.\\]\n\nFind the energy levels (eigenvalues) of \\(H\\).\nDetermine the stationary states (normalized eigenvectors).\nVerify algebraic and geometric multiplicities.\nExpress \\(H\\) using spectral decomposition and state how eigenvalues control state evolution phases.\n\n\n\nControl System — State Matrix Modes\n\nA linear time-invariant system has state matrix\n\\[A = \\begin{bmatrix}0 & 1 \\\\ -5 & -4\\end{bmatrix}.\\]\n\nFind the modes of system response (eigenvalues of \\(A\\)).\nDetermine the response directions (eigenvectors) and indicate whether they are real or complex.\nVerify algebraic and geometric multiplicities.\nInterpret the real parts of eigenvalues as stability margins and the imaginary parts as oscillation frequency.\n\n\n\n\n3.4.2 Solutions\n\n1. MEMS Resonator — Vibration Modes and Stability\n\n(a) Characteristic polynomial and eigenvalues.\nCompute \\(\\det(K-\\lambda I)=0\\). The characteristic polynomial simplifies to\n\\[-\\lambda^3 +16\\lambda^2 -76\\lambda +96 = 0,\\]\nwhose real positive roots are\n\\[\\lambda_1 = 2,\\quad \\lambda_2 = 6,\\quad \\lambda_3 = 8.\\]\n(b) Mode shapes (eigenvectors).\nSolve \\((K-\\lambda I)\\mathbf v=0\\) for each root:\n\nFor \\(\\lambda_1=2\\): \\(\\mathbf v_1 = [1,\\,2,\\,1]^T\\).\nFor \\(\\lambda_2=6\\): \\(\\mathbf v_2 = [-1,\\,0,\\,1]^T\\).\nFor \\(\\lambda_3=8\\): \\(\\mathbf v_3 = [1,\\,-1,\\,1]^T\\).\n\n(Students may present normalized forms.)\n\nPython code\n\n\n\nCode\nimport numpy as np\n\n# Stiffness matrix\nK = np.array([[6, -2, 0],\n              [-2, 4, -2],\n              [0, -2, 6]])\n\n# Eigenvalues and eigenvectors\neigvals, eigvecs = np.linalg.eig(K)\n\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Mode Shapes (eigenvectors):\")\nprint(eigvecs)\n\n\nEigenvalues: [2. 6. 8.]\nMode Shapes (eigenvectors):\n[[ 4.08248290e-01 -7.07106781e-01  5.77350269e-01]\n [ 8.16496581e-01  4.02240178e-16 -5.77350269e-01]\n [ 4.08248290e-01  7.07106781e-01  5.77350269e-01]]\n\n\n(c) Multiplicities.\nEach eigenvalue appears once in the characteristic polynomial (algebraic multiplicity = 1) and has one independent eigenvector (geometric multiplicity = 1).\n(d) Engineering interpretation.\nAssuming mass normalization, eigenvalues correspond to squared natural angular frequencies \\(\\omega^2\\). Therefore\n\\[\\omega_1=\\sqrt{2},\\;\\omega_2=\\sqrt{6},\\;\\omega_3=\\sqrt{8}.\\]\nAll eigenvalues positive and simple imply distinct, stable oscillatory modes; no zero/negative eigenvalues appear, so no rigid-body mode or instability is present.\n\n\n2. Wireless Channel Reliability — Long-term Measure & Convergence\n\n(a) Eigenvalues.\nSolve \\(\\det(P-\\lambda I)=0\\) to get\n\\[\\lambda_1 = 1,\\qquad \\lambda_2 = \\tfrac{1}{2}.\\]\n(b) Steady-state vector.\nSolve \\((P-I)\\mathbf v=0\\). One eigenvector is \\([1,1]^T\\), normalized to\n\\[\\pi = \\begin{bmatrix}1/2\\\\\\\\[4pt]1/2\\end{bmatrix}.\\]\n(c) Multiplicities.\nBoth eigenvalues are simple: algebraic multiplicity = 1 and geometric multiplicity = 1.\n(d) Engineering meaning.\n\\(\\lambda_1=1\\) yields the steady-state distribution (long-term reliability). The subdominant eigenvalue \\(\\lambda_2=0.5\\) determines exponential convergence speed: transients decay as \\((0.5)^t\\).\n\nPython code\n\n\n\nCode\nimport numpy as np\n\nA = np.array([[0.8, 0.3],\n              [0.2, 0.7]])\n\neigvals, eigvecs = np.linalg.eig(A)\n\nprint(\"Eigenvalues:\", eigvals)\nprint(\"Eigenvectors:\")\nprint(eigvecs)\n\n# Check stability\nstable = all(abs(eigvals) &lt; 1)\nprint(\"System stable?\", stable)\n\n\nEigenvalues: [1.  0.5]\nEigenvectors:\n[[ 0.83205029 -0.70710678]\n [ 0.5547002   0.70710678]]\nSystem stable? False\n\n\n\n\n3. PCA — Principal Directions and Variance Explained\n\n(a) Eigenvalues.\nSolve \\(\\det(C-\\lambda I)=0\\) to obtain\n\\[\\lambda_1 = 6,\\qquad \\lambda_2 = 1.\\]\n(b) Eigenvectors (principal directions).\n\nFor \\(\\lambda_1=6\\): \\(\\mathbf u_1=[2,\\,1]^T\\) (principal direction).\nFor \\(\\lambda_2=1\\): \\(\\mathbf u_2=[-1,\\,2]^T\\).\n\n(c) Multiplicities.\nDistinct eigenvalues ⇒ algebraic multiplicities = geometric multiplicities = 1.\n(d) Compression efficiency.\nTotal variance \\(=6+1=7\\). Fraction captured by first PC:\n\\[\\frac{6}{7}\\approx 85.71\\%.\\]\nProjecting onto the first principal direction retains ~85.7% of variance.\n\nPython code\n\n\n\nCode\nimport numpy as np\n\n# Covariance matrix from Question 3\nC = np.array([[5., 2.],\n              [2., 2.]])\n\n# eigendecomposition\neigvals, eigvecs = np.linalg.eig(C)\n\n# sort in descending order of eigenvalue\nidx = np.argsort(eigvals)[::-1]\neigvals = eigvals[idx]\neigvecs = eigvecs[:, idx]\n\n# normalize eigenvectors (columns) to unit length\neigvecs_norm = eigvecs / np.linalg.norm(eigvecs, axis=0)\n\nprint(\"Covariance matrix C:\\n\", C)\nprint(\"\\nEigenvalues (sorted):\", eigvals)\nprint(\"Normalized eigenvectors (columns correspond to eigenvalues):\\n\", eigvecs_norm)\n\n# Variance explained\ntotal_var = eigvals.sum()\nfractions = eigvals / total_var\nprint(\"\\nTotal variance: {:.6f}\".format(total_var))\nfor i,(lam, frac) in enumerate(zip(eigvals, fractions), start=1):\n    print(f\"PC{i}: eigenvalue = {lam:.6f}  -&gt; variance = {frac*100:.4f}%\")\n\n# Algebraic multiplicity (cluster equal eigenvalues within tol)\ndef algebraic_multiplicities(vals, tol=1e-8):\n    vals = list(vals)\n    used = [False]*len(vals)\n    clusters = []\n    for i,v in enumerate(vals):\n        if used[i]:\n            continue\n        cnt = 1\n        used[i] = True\n        for j in range(i+1, len(vals)):\n            if (not used[j]) and (abs(vals[j] - v) &lt; tol):\n                cnt += 1\n                used[j] = True\n        clusters.append((v, cnt))\n    return clusters\n\n# Geometric multiplicity = dimension of nullspace of (C - lambda I) = n - rank(C - lambda I)\ndef geometric_multiplicity(A, lam, tol=1e-10):\n    M = A - lam * np.eye(A.shape[0])\n    rank = np.linalg.matrix_rank(M, tol=tol)\n    return A.shape[0] - rank\n\nprint(\"\\nAlgebraic multiplicities (value, count):\", algebraic_multiplicities(eigvals))\nfor lam in np.unique(np.round(eigvals, 12)):\n    print(f\"Geometric multiplicity of λ={lam}: {geometric_multiplicity(C, lam)}\")\n\n# Helper: integer-proportional presentation of eigenvectors (useful for handouts)\ndef integer_representation(v, tol=1e-8):\n    # scale vector so the smallest non-zero absolute entry becomes 1 (or -1), then round\n    nonzero = np.abs(v) &gt; tol\n    if not nonzero.any():\n        return np.zeros_like(v, dtype=int)\n    scale = np.min(np.abs(v[nonzero]))\n    v_scaled = v / scale\n    v_int = np.round(v_scaled).astype(int)\n    return v_int\n\nprint(\"\\nInteger-proportional eigenvectors (for presentation):\")\nfor i in range(eigvecs_norm.shape[1]):\n    v = eigvecs_norm[:, i]\n    # fix sign convention for presentation: make first nonzero entry positive\n    first_nonzero = np.where(np.abs(v) &gt; 1e-8)[0][0]\n    if v[first_nonzero] &lt; 0:\n        v = -v\n    print(f\"λ = {eigvals[i]:.6f} : approx vector -&gt;\", integer_representation(v))\n\n\nCovariance matrix C:\n [[5. 2.]\n [2. 2.]]\n\nEigenvalues (sorted): [6. 1.]\nNormalized eigenvectors (columns correspond to eigenvalues):\n [[ 0.89442719 -0.4472136 ]\n [ 0.4472136   0.89442719]]\n\nTotal variance: 7.000000\nPC1: eigenvalue = 6.000000  -&gt; variance = 85.7143%\nPC2: eigenvalue = 1.000000  -&gt; variance = 14.2857%\n\nAlgebraic multiplicities (value, count): [(6.0, 1), (1.0, 1)]\nGeometric multiplicity of λ=1.0: 1\nGeometric multiplicity of λ=6.0: 1\n\nInteger-proportional eigenvectors (for presentation):\nλ = 6.000000 : approx vector -&gt; [2 1]\nλ = 1.000000 : approx vector -&gt; [ 1 -2]\n\n\n\n\n4. Quantum 2-Level System — Energy Levels & Stationary States\n\n(a) Eigenvalues (energy levels).\nSolve \\(\\det(H-\\lambda I)=0\\); roots are\n\\[\\lambda_1 = 3,\\quad \\lambda_2 = 5.\\]\n(b) Normalized stationary states.\n\nFor \\(\\lambda_1=3\\): \\(\\mathbf p_1 = \\tfrac{1}{\\sqrt{2}}[-1,\\,1]^T\\).\nFor \\(\\lambda_2=5\\): \\(\\mathbf p_2 = \\tfrac{1}{\\sqrt{2}}[1,\\,1]^T\\).\n\n(c) Multiplicities.\nBoth simple (algebraic = geometric = 1).\n(d) Spectral decomposition.\n\\[H = 3\\,\\mathbf p_1\\mathbf p_1^T + 5\\,\\mathbf p_2\\mathbf p_2^T.\\]\nTime evolution \\(e^{-iHt}\\) applies phases \\(e^{-i\\lambda t}\\) to each stationary state.\n\nPython code\n\n\n\nCode\nfrom scipy.linalg import expm\n\nH = np.array([[4, 1],\n              [1, 4]])\n\neigvals, eigvecs = np.linalg.eig(H)\n\nprint(\"Eigenvalues (energy levels):\", eigvals)\nprint(\"Eigenvectors (stationary states):\\n\", eigvecs)\n\n# Verify spectral decomposition\nH_recon = eigvecs @ np.diag(eigvals) @ np.linalg.inv(eigvecs)\nprint(\"Reconstructed H:\\n\", np.round(H_recon, 5))\n\n# Time evolution operator at t=1, ħ=1\nt = 1.0\nU = expm(-1j * H * t)\nprint(\"Time evolution operator U(t=1):\\n\", np.round(U, 4))\n\n\nEigenvalues (energy levels): [5. 3.]\nEigenvectors (stationary states):\n [[ 0.70710678 -0.70710678]\n [ 0.70710678  0.70710678]]\nReconstructed H:\n [[4. 1.]\n [1. 4.]]\nTime evolution operator U(t=1):\n [[-0.3532+0.4089j  0.6368+0.55j  ]\n [ 0.6368+0.55j   -0.3532+0.4089j]]\n\n\n\n\n\nControl System — Modes & Stability Margin\n\n\n(a) Eigenvalues (modes).\nCharacteristic equation \\(\\lambda^2 + 4\\lambda + 5 = 0\\) gives\n\\[\\lambda_{1,2} = -2 \\pm i.\\]\n(b) Eigenvectors.\nCorresponding complex eigenvectors can be computed; one representative for \\(\\lambda=-2+i\\) is\n\\[\\mathbf v = \\begin{bmatrix}-\\tfrac{2}{5}+\\tfrac{i}{5}\\\\\\\\[4pt] 1 \\end{bmatrix}.\\]\nReal sinusoidal modes can be constructed from the complex eigenpairs.\n(c) Multiplicities.\nDistinct complex conjugate eigenvalues → algebraic multiplicity = geometric multiplicity = 1 for each.\n(d) Interpretation.\nReal part \\(-2\\) indicates exponential decay (stable). Imaginary part \\(\\pm1\\) indicates oscillation frequency 1 rad/s. Decay envelope ~ \\(e^{-2t}\\).\n\nPython code\n\n\n\nCode\n# Control system (2x2) analysis: A = [[0,1],[-5,-4]]\nimport numpy as np\nfrom scipy.linalg import expm\n\nA = np.array([[0., 1.],\n              [-5., -4.]])\n\n# Eigen-decomposition\neigvals, eigvecs = np.linalg.eig(A)   # eigvecs columns correspond to eigvals\n\nprint(\"A =\\n\", A)\nprint(\"\\nEigenvalues:\")\nfor i, lam in enumerate(eigvals, start=1):\n    print(f\"  λ{i} = {lam}\")\n\nprint(\"\\nEigenvectors (columns):\\n\", eigvecs)\n\n# Real and imaginary parts\nprint(\"\\nReal parts of eigenvalues:\", np.real(eigvals))\nprint(\"Imag parts of eigenvalues:\", np.imag(eigvals))\n\n# Stability check: asymptotically stable if all real parts &lt; 0\nis_stable = np.all(np.real(eigvals) &lt; 0)\nprint(\"\\nAsymptotically stable?\", is_stable)\n\n# Algebraic multiplicities (count equal eigenvalues within tolerance)\ndef algebraic_multiplicities(vals, tol=1e-8):\n    vals_r = np.round(vals.real, 8) + 1j*np.round(vals.imag, 8)\n    unique, counts = np.unique(vals_r, return_counts=True)\n    return list(zip(unique, counts))\n\nprint(\"\\nAlgebraic multiplicities:\", algebraic_multiplicities(eigvals))\n\n# Geometric multiplicity: n - rank(A - λI)\ndef geometric_multiplicity(A, lam, tol=1e-10):\n    M = A - lam * np.eye(A.shape[0])\n    rank = np.linalg.matrix_rank(M, tol=tol)\n    return A.shape[0] - rank\n\nfor lam in eigvals:\n    gm = geometric_multiplicity(A, lam)\n    print(f\"Geometric multiplicity of λ={lam}: {gm}\")\n\n# Check diagonalizability (geometric multiplicities == algebraic)\n# For small matrices we can check if eigvecs are linearly independent:\nrank_eigvecs = np.linalg.matrix_rank(eigvecs)\ndiag_ok = (rank_eigvecs == A.shape[0])\nprint(\"\\nEigenvector matrix rank:\", rank_eigvecs)\nprint(\"Matrix diagonalizable over C?\", diag_ok)\n\n# Spectral reconstruction: A_recon = V * diag(lambda) * V^{-1}\nA_recon = eigvecs @ np.diag(eigvals) @ np.linalg.inv(eigvecs)\nprint(\"\\nSpectral reconstruction (A_recon):\\n\", np.round(A_recon, 10))\n\n# If eigenvalues are complex conjugates, form real modal (real-valued 2x2 block) representation\n# and compute the real state transition e^{At} via expm\nt = 0.5  # example time\nPhi = expm(A * t)\nprint(f\"\\nState-transition matrix e^{{A t}} for t={t}:\\n\", np.round(Phi, 6))\n\n# Provide a short interpretation printout\nprint(\"\\nInterpretation:\")\nprint(\" - Eigenvalues provide modes: real part =&gt; decay rate, imag part =&gt; oscillation frequency.\")\nprint(\" - Here, eigenvalues = -2 ± 1j  =&gt; decaying oscillation with decay rate 2 and angular freq 1.\")\n\n\nA =\n [[ 0.  1.]\n [-5. -4.]]\n\nEigenvalues:\n  λ1 = (-2.0000000000000004+1.0000000000000004j)\n  λ2 = (-2.0000000000000004-1.0000000000000004j)\n\nEigenvectors (columns):\n [[-0.36514837-0.18257419j -0.36514837+0.18257419j]\n [ 0.91287093+0.j          0.91287093-0.j        ]]\n\nReal parts of eigenvalues: [-2. -2.]\nImag parts of eigenvalues: [ 1. -1.]\n\nAsymptotically stable? True\n\nAlgebraic multiplicities: [((-2-1j), 1), ((-2+1j), 1)]\nGeometric multiplicity of λ=(-2.0000000000000004+1.0000000000000004j): 1\nGeometric multiplicity of λ=(-2.0000000000000004-1.0000000000000004j): 1\n\nEigenvector matrix rank: 2\nMatrix diagonalizable over C? True\n\nSpectral reconstruction (A_recon):\n [[-0.+0.j  1.-0.j]\n [-5.-0.j -4.-0.j]]\n\nState-transition matrix e^{A t} for t=0.5:\n [[ 0.675586  0.176371]\n [-0.881854 -0.029897]]\n\nInterpretation:\n - Eigenvalues provide modes: real part =&gt; decay rate, imag part =&gt; oscillation frequency.\n - Here, eigenvalues = -2 ± 1j  =&gt; decaying oscillation with decay rate 2 and angular freq 1.\n\n\n\n\nNotes for instructors\n\n\nThe answers above use convenient integer eigenvectors; students should be awarded full credit for any nonzero scalar multiple or normalized versions.\nEncourage explicit algebraic steps: expansion of determinants, solving the linear systems for eigenvectors, and normalization when required.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#diagonalization-of-a-matrix",
    "href": "module2.html#diagonalization-of-a-matrix",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.5 Diagonalization of a Matrix",
    "text": "3.5 Diagonalization of a Matrix\nThis brings us to one of the most powerful ideas in linear algebra: diagonalization. The goal is to write a matrix \\(A\\) as a product of three simpler matrices.\n\nThe Diagonalization Formula \\[ A = S \\Lambda S^{-1} \\] Where: * \\(S\\) is the matrix whose columns are the eigenvectors of \\(A\\). * \\(\\Lambda\\) (Lambda) is the diagonal matrix with the eigenvalues on its diagonal.\n\nA matrix can be diagonalized if and only if it has a full set of linearly independent eigenvectors.\nWhy is this so useful? Consider computing \\(A^{100}\\). This would be a nightmare. But if \\(A\\) is diagonalized: \\[ A^2 = (S \\Lambda S^{-1})(S \\Lambda S^{-1}) = S \\Lambda (S^{-1}S) \\Lambda S^{-1} = S \\Lambda^2 S^{-1} \\] In general: \\[ A^k = S \\Lambda^k S^{-1} \\] Computing \\(\\Lambda^k\\) is trivial: you just raise the diagonal entries to the \\(k\\)-th power!\n\n3.5.1 Example: Verifying Diagonalization\nLet’s verify \\(A = S \\Lambda S^{-1}\\) and compute \\(A^3\\) for our matrix.\n\n\nCode\nimport numpy as np\n\n# Our matrix A\nA = np.array([\n    [4, -2],\n    [1,  1]\n], dtype=float)  # ensure float for safety in inverse\n\n# NumPy's eig function gives both eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# S is the matrix of eigenvectors\nS = eigenvectors\n# Λ is the diagonal matrix of eigenvalues\nLambda = np.diag(eigenvalues)\n\nprint(\"Matrix S (Eigenvectors):\")\nprint(S)\nprint(\"\\nMatrix Λ (Eigenvalues):\")\nprint(Lambda)\n\n# Calculate S-inverse\nS_inv = np.linalg.inv(S)\n\n# Verify A = SΛS⁻¹\nA_reconstructed = S @ Lambda @ S_inv\nprint(\"\\nReconstructed A = SΛS⁻¹:\")\nprint(A_reconstructed)\nprint(\"\\nIs it close to the original A?\", np.allclose(A, A_reconstructed))\n\n# Compute A³ directly\nA_cubed_direct = np.linalg.matrix_power(A, 3)\n\n# Compute A³ using diagonalization\nLambda_cubed = np.diag(eigenvalues**3)\nA_cubed_diagonal = S @ Lambda_cubed @ S_inv\n\nprint(\"\\n--- Computing A³ ---\")\nprint(\"Direct computation (A³):\")\nprint(A_cubed_direct)\nprint(\"\\nUsing diagonalization (SΛ³S⁻¹):\")\nprint(A_cubed_diagonal)\nprint(\"\\nAre the results close?\", np.allclose(A_cubed_direct, A_cubed_diagonal))\n\n\nMatrix S (Eigenvectors):\n[[0.89442719 0.70710678]\n [0.4472136  0.70710678]]\n\nMatrix Λ (Eigenvalues):\n[[3. 0.]\n [0. 2.]]\n\nReconstructed A = SΛS⁻¹:\n[[ 4. -2.]\n [ 1.  1.]]\n\nIs it close to the original A? True\n\n--- Computing A³ ---\nDirect computation (A³):\n[[ 46. -38.]\n [ 19. -11.]]\n\nUsing diagonalization (SΛ³S⁻¹):\n[[ 46. -38.]\n [ 19. -11.]]\n\nAre the results close? True",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#symmetric-and-orthogonal-matrices",
    "href": "module2.html#symmetric-and-orthogonal-matrices",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.6 Symmetric and Orthogonal Matrices",
    "text": "3.6 Symmetric and Orthogonal Matrices\nThings get even nicer for a very special class of matrices: symmetric matrices, where \\(A = A^T\\).\nSymmetric matrices have two incredible properties: 1. All their eigenvalues are real numbers. 2. Their eigenvectors can be chosen to be orthogonal (perpendicular to each other).\nIf we normalize the eigenvectors (make them unit length), they form an orthonormal set. The matrix \\(Q\\) whose columns are these orthonormal eigenvectors is an orthogonal matrix.\nOrthogonal matrices are wonderful because their inverse is simply their transpose: \\(Q^{-1} = Q^T\\).\nThis leads to the Spectral Theorem, which says that any symmetric matrix can be diagonalized as: \\[ A = Q \\Lambda Q^T \\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#application-quadratic-forms",
    "href": "module2.html#application-quadratic-forms",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.7 Application: Quadratic Forms",
    "text": "3.7 Application: Quadratic Forms\nWhat are these ideas good for? One classic application is understanding the geometry of quadratic forms. A quadratic form is a polynomial where every term has degree two. For example: \\[ f(x, y) = 2x^2 + 6xy + 2y^2 \\] This equation describes a shape on the plane. But what shape? The \\(6xy\\) “cross-product” term makes it hard to see because it corresponds to a rotated shape. Our goal is to eliminate this term.\nWe can write any quadratic form using a symmetric matrix: \\[ f(x, y) = \\begin{bmatrix} x & y \\end{bmatrix} \\begin{bmatrix} 2 & 3 \\\\ 3 & 2 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x^T A x \\]\nThe eigenvectors of \\(A\\) point along the principal axes of the shape, and the eigenvalues tell us the scaling in those directions. By changing to a coordinate system aligned with the eigenvectors, we can describe the shape without a cross-term.\nLet’s find the axes of the ellipse given by \\(2x^2 + 6xy + 2y^2 = 1\\).\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Symmetric matrix for the quadratic form\nA = np.array([\n    [3, 1],\n    [1, 2]\n])\n\n# Eigen-decomposition gives the axes and scaling\neigvals, eigvecs = np.linalg.eig(A)\nv1 = eigvecs[:, 0]\nv2 = eigvecs[:, 1]\n\n# Generate grid data\nx = np.linspace(-1.5, 1.5, 400)\ny = np.linspace(-1.5, 1.5, 400)\nX, Y = np.meshgrid(x, y)\n\n# Quadratic form value: F(x, y) = [x y] A [x; y]\nF = A[0, 0] * X**2 + 2 * A[0, 1] * X * Y + A[1, 1] * Y**2\n\n# Plotting\nplt.figure(figsize=(7, 7))\n\n# Draw the level set F = 1 (ellipse)\nplt.contour(X, Y, F, levels=[1], colors='blue')\n\n# Draw eigenvectors scaled by semi-axis lengths\naxis1 = v1 / np.sqrt(eigvals[0])\naxis2 = v2 / np.sqrt(eigvals[1])\n\nplt.quiver(0, 0, axis1[0], axis1[1], angles='xy', scale_units='xy', scale=1, color='red', label=f'Axis 1 (λ={eigvals[0]:.2f})')\nplt.quiver(0, 0, -axis1[0], -axis1[1], angles='xy', scale_units='xy', scale=1, color='red')\n\nplt.quiver(0, 0, axis2[0], axis2[1], angles='xy', scale_units='xy', scale=1, color='red', label=f'Axis 2 (λ={eigvals[1]:.2f})')\nplt.quiver(0, 0, -axis2[0], -axis2[1], angles='xy', scale_units='xy', scale=1, color='red')\n\n# Formatting\nplt.title('Principal Axes of a Quadratic Form')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid(True)\nplt.axhline(0, color='black', linewidth=0.5)\nplt.axvline(0, color='black', linewidth=0.5)\nplt.legend()\nplt.axis('equal')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3.2: The ellipse defined by the quadratic form. The eigenvectors (red) point along the major and minor axes.\n\n\n\n\n\nThe plot shows it perfectly. The quadratic form describes a rotated ellipse, and the eigenvectors of the associated symmetric matrix point exactly along its major and minor axes. The signs of the eigenvalues (\\(\\lambda_1=5, \\lambda_2=-1\\)) tell us it’s a hyperbola (one positive, one negative), not an ellipse. My description was slightly off, but the math and the plot reveal the truth! This is why we do the computation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  },
  {
    "objectID": "module2.html#module-ii-summary",
    "href": "module2.html#module-ii-summary",
    "title": "3  Module-2: Matrix Eigen Value Problems",
    "section": "3.8 Module II Summary",
    "text": "3.8 Module II Summary\n\nThe core idea of this module is the equation \\(Ax = \\lambda x\\).\nEigenvectors (\\(x\\)) are the special directions for a matrix that are only scaled, not rotated. Eigenvalues (\\(\\lambda\\)) are the scaling factors.\nWe find eigenvalues by solving the characteristic equation \\(\\det(A - \\lambda I) = 0\\).\nDiagonalization (\\(A = S \\Lambda S^{-1}\\)) is a powerful tool that simplifies matrix powers and reveals the true nature of a transformation.\nSymmetric matrices are the nicest of all, with real eigenvalues and orthogonal eigenvectors, leading to the decomposition \\(A = Q \\Lambda Q^T\\).\nThis theory has direct applications in geometry, such as finding the principal axes of shapes defined by quadratic forms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Module-2: Matrix Eigen Value Problems</span>"
    ]
  }
]