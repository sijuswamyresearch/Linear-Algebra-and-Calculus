---
title: "Linear Algebra into Action"
subtitle: "From Theory to Application"
author: "Siju Swamy"
institute: "Department of Mathematics"
date: today

format: 
  revealjs:
    theme: [simple, default]       # clean professional theme
    transition: fade               # smooth transition
    background-transition: fade
    slide-number: true             # small slide number at bottom
    logo: logo.png                 # optional logo in corner
    footer: "Electronics & Computational Mathematics | 2025"
    highlight-style: github        # code highlighting
    code-line-numbers: true        # professional style for code
    toc: false                     # no cluttered TOC
    incremental: false             # bullet points appear step by step
    preview-links: auto
    css: slides.css                # custom css for colors
jupyter: python3
code-fold: true
---

## Robotics- the technology for live agents

<iframe width="560" height="315" src="https://www.youtube.com/embed/fn3KWM1kuAw?si=dgFy3hM86mhYGXuF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>


## Why Linear Algebra & Calculus in Electronics/ Computer Science?

::: incremental
- **Linear Algebra** â†’ graphics, data science, signal processing, circuit analysis
- **Calculus** â†’ optimization, machine learning, control systems, circuit dynamics
- **Differential Equations** â†’ modeling system behavior, power electronics
- **Optimization** â†’ algorithm efficiency, network routing, AI models
:::

## Introduction

- This session connects **Linear Algebra** with **Electronics & Computer Science**
- Focus: Using matrices for **data manipulation & circuit analysis**
- Students will see:
  - How **vector spaces** underpin data science and machine learning
  - Applications in **signal processing** and **graph theory**
  - Solving **circuit equations** with matrices in **Python**

## Core Concepts in ECS  

- **Data Representation**: How numbers, images, and signals are structured.
- **System Modeling**: Describing the behavior of circuits, networks, and algorithms.

## Data & Signals vs. System Dynamics

## Linear Algebra in ECS

::: {.smaller}
| Aspect        | Data & Signals      | System Dynamics   |
|---------------|---------------------|-------------------|
| **Input**     | Raw data, sensor readings | Initial states, inputs |
| **Output**    | Features, processed signals | System response, outputs |
| **Method**    | Matrix transformations, SVD | State-space models, ODEs |
| **Difficulty**| Feature engineering, dimensionality reduction | Stability analysis, control design |
:::

## Why Important?  

- Machine Learning & AI (neural networks, PCA)  
- Computer Graphics (transformations, projections)  
- Signal Processing (filtering, image compression)  
- Circuit Design & Analysis (KVL/KCL, network theory)  
- Algorithm Optimization & Data Structures  

## Python implementation of 2D Graphics Transform

```{python}
#| code-fold: true
#| echo: true
import numpy as np
import matplotlib.pyplot as plt

def create_transform_matrix(scale_x, scale_y, translate_x, translate_y):
    # Scale matrix
    S = np.array([
        [scale_x, 0, 0],
        [0, scale_y, 0],
        [0, 0, 1]
    ])
    # Translate matrix
    T = np.array([
        [1, 0, translate_x],
        [0, 1, translate_y],
        [0, 0, 1]
    ])
    return np.dot(T, S) # Apply scale, then translate

# Define a simple square (homogeneous coordinates)
square_points = np.array([
    [0, 1, 1, 0, 0],  # X coordinates
    [0, 0, 1, 1, 0],  # Y coordinates
    [1, 1, 1, 1, 1]   # Homogeneous coordinate
])

# Create a transform: scale by 2x, then translate by (1, 0.5)
transform_matrix = create_transform_matrix(2, 2, 1, 0.5)

# Apply the transformation
transformed_points = np.dot(transform_matrix, square_points)

# Plotting
fig, ax = plt.subplots(figsize=(6,6))
ax.plot(square_points[0, :], square_points[1, :], 'b-o', label='Original')
ax.plot(transformed_points[0, :], transformed_points[1, :], 'r-x', label='Transformed')
ax.set_xlim(-1, 4)
ax.set_ylim(-1, 4)
ax.set_aspect('equal')
ax.set_title("2D Graphics Transformation")
ax.legend()
plt.grid(True)
plt.show()
```

## Role of Rank in ECS

:::{.smaller}
| Concept | ECS Meaning | Effect of Rank Deficiency |
|---------------------|--------------------------------------------------|--------------------------------------|
| Matrix Rank | Number of independent equations/features | Loss of information, underdetermined systems |
| Linear Independence | Features/signals provide unique information | Redundancy, reduced efficiency |
| Singular Value Decomposition (SVD) | Basis for dimensionality reduction, data compression | Highlights principal components |
:::

## Rank and Data Redundancy

In data science, a low-rank matrix implies that data features are highly correlated or redundant.
```{python}
#| code-fold: true
#| echo: true
import numpy as np

# Example 1: Full Rank (independent features)
data1 = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9.1] # slightly perturbed to ensure full rank
])
print("Data Matrix 1 (Full Rank):")
print(data1)
print("Rank:", np.linalg.matrix_rank(data1))

# Example 2: Rank Deficient (redundant feature - col 3 is col 2 + col 1)
data2 = np.array([
    [1, 2, 3],
    [2, 4, 6],
    [3, 6, 9]
])
print("\nData Matrix 2 (Rank Deficient):")
print(data2)
print("Rank:", np.linalg.matrix_rank(data2))

# Example 3: SVD for dimensionality reduction insights
U, s, Vt = np.linalg.svd(data2)
print("\nSingular Values of Data Matrix 2:", s)
# The number of non-zero singular values equals the rank.
```

## Forward Kinematics â€” 2D Robot (Geometry)

::: {.center}
![](2D-FK.png){width=90%}
:::


## Homogeneous transforms (2D)

For a rotation by $\theta$ and translation along x by $a$:

$$
R(\theta) =
\begin{bmatrix}
\cos\theta & -\sin\theta & 0\\[4pt]
\sin\theta & \cos\theta  & 0\\[4pt]
0 & 0 & 1
\end{bmatrix}
\qquad
T_x(a) =
\begin{bmatrix}
1 & 0 & a\\[4pt]
0 & 1 & 0\\[4pt]
0 & 0 & 1
\end{bmatrix}
$$

Link transform (rotate then translate along local x): $T_i = R(\theta_i)\,T_x(L_i)$

## Matrix representation of Forward Kinematics

$$
T=\begin{bmatrix}
\cos\theta_1 & -\sin\theta_1 & L_1\cos\theta_1 \\
\sin\theta_1 & \cos\theta_1 & L_1\sin\theta_1 \\
0 & 0 & 1
\end{bmatrix}\cdot \begin{bmatrix}
\cos\theta_2 & -\sin\theta_2 & L_2\cos\theta_2 \\
\sin\theta_2 & \cos\theta_2 & L_2\sin\theta_2 \\
0 & 0 & 1
\end{bmatrix}
$$

$$
T =
\begin{bmatrix}
\cos(\theta_1+\theta_2) & -\sin(\theta_1+\theta_2) &  L_1\cos\theta_1 + L_2\cos(\theta_1+\theta_2) \\
\sin(\theta_1+\theta_2) & \cos(\theta_1+\theta_2) &  L_1\sin\theta_1 + L_2\sin(\theta_1+\theta_2) \\
0 & 0 & 1
\end{bmatrix}
$$


## Python implementation of a 2D Robot arm

```{python}
#| code-fold: true
#| echo: true
# Forward Kinematics for a 2-link planar robot arm
import numpy as np
import matplotlib.pyplot as plt

def fk(theta1, theta2, L1=1.0, L2=1.0):
    x1, y1 = L1*np.cos(theta1), L1*np.sin(theta1)
    x2, y2 = x1 + L2*np.cos(theta1+theta2), y1 + L2*np.sin(theta1+theta2)
    return (0,0), (x1,y1), (x2,y2)

p0, p1, p2 = fk(np.pi/4, np.pi/6)

fig = plt.figure(figsize=(5,5))
ax = fig.add_subplot(111)
ax.plot([p0[0], p1[0], p2[0]], [p0[1], p1[1], p2[1]], '-o', linewidth=3)
ax.set_xlim(-2,2)
ax.set_ylim(-2,2)
ax.set_aspect('equal')
ax.set_title("2-Link Planar Robot Arm")
plt.show()
```

## Role of Rank in Robotics

:::{.smaller}
| Concept             | Robotics Meaning                                   | Effect of Rank Deficiency            |
|---------------------|----------------------------------------------------|--------------------------------------|
| **Jacobian Matrix** | Relates joint velocities â†’ end-effector velocities | Lose control in some directions       |
| **DOF** | Independent motions of robot joints           | Reduced mobility                      |
| **Singularity**     | Robot in stretched/folded pose                     | Infinite joint velocities, instability |
:::


## Rank and Singularity in Robotics

At certain configurations, the Jacobian matrix loses rank â†’  robot loses degrees of freedom.

```{python}
#| code-fold: true
#| echo: true
import numpy as np

def jacobian(theta1, theta2, l1=1.0, l2=1.0):
    # Forward kinematics: end-effector (x,y)
    x = l1*np.cos(theta1) + l2*np.cos(theta1+theta2)
    y = l1*np.sin(theta1) + l2*np.sin(theta1+theta2)
    
    # Jacobian matrix
    J = np.array([
        [-l1*np.sin(theta1) - l2*np.sin(theta1+theta2), -l2*np.sin(theta1+theta2)],
        [ l1*np.cos(theta1) + l2*np.cos(theta1+theta2),  l2*np.cos(theta1+theta2)]
    ])
    
    return (x, y), J

# Example 1: Normal configuration
(x1, y1), J1 = jacobian(np.pi/4, np.pi/4)
print("Configuration 1 (Normal):")
print("Jacobian:\n", J1)
print("Rank:", np.linalg.matrix_rank(J1))

# Example 2: Singularity (arm fully stretched)
(x2, y2), J2 = jacobian(0, 0)
print("\nConfiguration 2 (Singularity):")
print("Jacobian:\n", J2)
print("Rank:", np.linalg.matrix_rank(J2))
```

## Solving System of Equations

In robotics, joint positions and constraints often lead to a **system of linear equations**:  

- Suppose the arm must satisfy **linear constraints**:
$x + y = d, \quad 2x - y = c$

- Matrix form:
$$
\underbrace{\begin{bmatrix}
1 & 1 \\
2 & -1
\end{bmatrix}}_{A}
\begin{bmatrix} x \\ y \end{bmatrix}
=
\underbrace{\begin{bmatrix} d \\ c \end{bmatrix}}_{b}
$$

## Python code for solution

```{python}
#| code-fold: true
#| echo: true
import numpy as np

# Coefficient matrix A
A = np.array([[1, 1],
              [2, -1]])

# Constants (d=5, c=1)
b = np.array([5, 1])

# Solve Ax = b
solution = np.linalg.solve(A, b)

x, y = solution
print(f"Solution: x = {x:.2f}, y = {y:.2f}")
```

## Looking Ahead

ðŸš€ Future sessions will bring:  

- Hands-on computational experiments with real datasets
- Step-by-step application of Linear Algebra in circuit simulations and AI models
- A clear path from mathematical concepts â†’ practical ECS solutions
 

**"The computational aspect will make the mathematics come alive."**