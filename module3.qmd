---
title: "Module 3: Multivariable Calculus - Differentiation"
jupyter: python3
---
>**Syllabus:** Multivariable Calculus - Differentiation
Concept of limit and continuity of functions of two variables - Partial derivatives of
first and higher order - Implicit partial differentiation - Local linear approximations -
Chain rule for derivatives and partial derivatives - Relative maxima and minima of
function of two variables (finding relative extrema only)

----
## A New Dimension

So far, our world has been one of lines, planes, and vectors—the "flat" world of linear algebra. Now, we venture into the "curvy" world of calculus, but in higher dimensions.

In single-variable calculus, you studied functions $y = f(x)$, whose graphs are curves in a 2D plane. Now, we'll explore functions of two variables, $z = f(x, y)$. Their graphs are **surfaces** in 3D space.

Think of it like this: $x$ and $y$ are your coordinates on a map (east-west and north-south), and $z$ is your altitude. The function $f(x, y)$ describes a landscape. Our goal is to understand this landscape: how steep is it? Which way is uphill? Where are the peaks and valleys?

Let's start by looking at a landscape.

```{python}
#| label: fig-surface-plot
#| fig-cap: "An interactive plot of the surface z = f(x,y). Our goal is to analyze its features."

import numpy as np
import plotly.graph_objects as go

# Define the function that describes our "landscape"
def f(x, y):
    return (x**2 + 3*y**2) * np.exp(1 - x**2 - y**2)

# Create a grid of (x,y) points
x_vals = np.linspace(-2.5, 2.5, 100)
y_vals = np.linspace(-2.5, 2.5, 100)
X, Y = np.meshgrid(x_vals, y_vals)

# Calculate the z-value (altitude) for each point
Z = f(X, Y)

# Create the interactive 3D surface plot
fig = go.Figure(data=[go.Surface(z=Z, x=X, y=Y)])

fig.update_layout(
    title='The "Landscape" of a Function of Two Variables',
    scene=dict(
        xaxis_title='x-axis',
        yaxis_title='y-axis',
        zaxis_title='z-axis (Altitude)'
    ),
    width=800, height=600,
    autosize=False
)

fig.show()
```

Looking at this plot, we can see peaks, a valley at the center, and ridges. How can we find these features mathematically?

## Partial Derivatives: The Slope in One Direction

How do we measure the "slope" of a surface? The problem is that the slope depends on which direction you're facing!

The simplest way to start is to do the simplest thing: **hold one variable constant**.

1.  Imagine you are standing on the surface and decide to walk *only* in the x-direction (due east). The slope you experience is the **partial derivative with respect to x**, written as $\frac{\partial f}{\partial x}$ or $f_x$.
2.  Alternatively, if you walk *only* in the y-direction (due north), the slope is the **partial derivative with respect to y**, written as $\frac{\partial f}{\partial y}$ or $f_y$.

To calculate $\frac{\partial f}{\partial x}$, you simply treat $y$ as a constant and differentiate with respect to $x$. Let's use `SymPy` to do this for our function $f(x, y) = x^2 e^{-y}$.

```{python}
#| label: partial-derivatives-sympy
import sympy as sp

# Define x and y as symbolic variables
x, y = sp.symbols('x y')

# Define a simpler function symbolically
f_sym = x**2 * sp.exp(-y)

print(f"Our function is f(x, y) = {f_sym}")

# Calculate the partial derivative with respect to x (treat y as a constant)
fx = sp.diff(f_sym, x)
print(f"The partial derivative ∂f/∂x is: {fx}")

# Calculate the partial derivative with respect to y (treat x as a constant)
fy = sp.diff(f_sym, y)
print(f"The partial derivative ∂f/∂y is: {fy}")

# We can also find second-order derivatives
fxx = sp.diff(fx, x)
fxy = sp.diff(fx, y)
print(f"\nThe second-order partial f_xx is: {fxx}")
print(f"The mixed partial f_xy is: {fxy}")
```

## The Gradient and Linear Approximation

The two partial derivatives tell us the slope in the cardinal directions. But what if we want to know the slope in *any* direction? We can package our partial derivatives into a single, powerful object: the **gradient vector**.

> **Definition: The Gradient**
> The gradient of $f(x,y)$ is the vector:
> $$ \nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix} = f_x \mathbf{i} + f_y \mathbf{j} $$

The gradient is not just a container. It has a beautiful geometric meaning:
1.  **Direction:** The gradient vector $\nabla f$ at a point $(x_0, y_0)$ points in the direction of the **steepest ascent** on the surface. It's the "uphill" direction.
2.  **Magnitude:** The magnitude of the gradient, $||\nabla f||$, is the slope in that steepest direction.

This leads to the idea of **local linear approximation**. Just as a smooth curve looks like its tangent line up close, a smooth surface looks like its **tangent plane** up close. The gradient helps us define this plane.

## Local Linear Approximation: The Tangent Plane

This is a central idea that connects everything together. Remember from single-variable calculus the **Madhava-Taylor series**. The first-order approximation of a function $f(x)$ near a point $x=a$ is its tangent line:
$$ L(x) = f(a) + f'(a)(x-a) $$
This is the **local linear approximation**. The idea is powerful: if you zoom in far enough on any smooth curve, it looks like a straight line.

We will now extend this to two dimensions. If you zoom in far enough on any smooth surface, it looks like a flat **plane**. This is the **tangent plane**.

> **Definition: Local Linear Approximation**
> The local linear approximation of a function $f(x,y)$ at a point $(a,b)$ is given by:
> $$ L(x, y) = f(a, b) + f_x(a, b)(x-a) + f_y(a, b)(y-b) $$
> The graph of this function, $z = L(x,y)$, is the **tangent plane** to the surface $z = f(x,y)$ at the point $(a,b)$.

This formula is a beautiful extension of the 1D case. It says the approximate height near $(a,b)$ is the starting height $f(a,b)$, plus the change due to moving in x (slope in x times distance in x), plus the change due to moving in y (slope in y times distance in y).

### Problem and Application: Estimating Values

Let's find the tangent plane for the function $f(x,y) = \sqrt{x^2 + y^2}$ (a cone) at the point $(3, 4)$ and use it to approximate $f(3.01, 3.99)$.

```{python}
#| label: linear-approx-problem
#| fig-cap: "Using SymPy to build the linear approximation L(x,y)."
import sympy as sp

# Define symbols and the function
x, y = sp.symbols('x y')
f = sp.sqrt(x**2 + y**2)
a, b = 3, 4

# 1. Find the value of the function at (a,b)
f_val = f.subs([(x, a), (y, b)])
print(f"The value f({a},{b}) is: {f_val}")

# 2. Find the partial derivatives
fx = sp.diff(f, x)
fy = sp.diff(f, y)
print(f"∂f/∂x = {fx}")
print(f"∂f/∂y = {fy}")

# 3. Find the slope values at (a,b)
fx_val = fx.subs([(x, a), (y, b)])
fy_val = fy.subs([(x, a), (y, b)])
print(f"\nThe slope fx({a},{b}) is: {fx_val}")
print(f"The slope fy({a},{b}) is: {fy_val}")

# 4. Assemble the linear approximation L(x,y)
L = f_val + fx_val * (x - a) + fy_val * (y - b)
print(f"\nThe Tangent Plane equation is: z = {sp.simplify(L)}")

# 5. Use L to approximate f(3.01, 3.99)
approx_val = L.subs([(x, 3.01), (y, 3.99)])
print(f"\nThe approximate value of f(3.01, 3.99) is: {approx_val}")

# 6. Compare with the true value
true_val = f.subs([(x, 3.01), (y, 3.99)])
print(f"The true value is: {true_val.evalf()}")
print(f"The approximation is excellent!")

```

### Visualization: Surface and Tangent Plane

Seeing is believing. Let's plot the cone and its tangent plane at $(3, 4, 5)$. Notice how the plane perfectly "kisses" the surface at that single point.

```{python}
#| label: fig-tangent-plane
#| fig-cap: "The tangent plane (red) provides a linear approximation to the surface (blue) at the point of tangency."

import numpy as np
import plotly.graph_objects as go

# Define the surface function
def f_np(x, y):
    return np.sqrt(x**2 + y**2)

# Create grid for the surface plot
x_surf = np.linspace(0, 6, 50)
y_surf = np.linspace(0, 8, 50)
X_surf, Y_surf = np.meshgrid(x_surf, y_surf)
Z_surf = f_np(X_surf, Y_surf)

# Tangent plane: L(x,y) = 5 + (3/5)(x-3) + (4/5)(y-4)
def L_np(x, y):
    return 5 + (3/5)*(x - 3) + (4/5)*(y - 4)

X_plane, Y_plane = np.meshgrid(np.linspace(1, 5, 10), np.linspace(2, 6, 10))
Z_plane = L_np(X_plane, Y_plane)

# Create the plot
fig = go.Figure()

# Add the surface
fig.add_trace(go.Surface(z=Z_surf, x=X_surf, y=Y_surf, opacity=0.8, name='f(x,y)'))

# Add the tangent plane
fig.add_trace(go.Surface(z=Z_plane, x=X_plane, y=Y_plane,
                         colorscale='Reds', showscale=False, name='Tangent Plane'))

# Add the point of tangency (3,4,5)
fig.add_trace(go.Scatter3d(
    x=[3], y=[4], z=[5],
    mode='markers',
    marker=dict(size=8, color='black'),
    name='Point (3,4,5)'
))

fig.update_layout(title='Surface and its Tangent Plane',
                  width=800, height=600, autosize=False)
fig.show()

```


## The Chain Rule: Derivatives on a Path

What if you're not standing still, but walking along a path on the map? Suppose your path is given by $(x(t), y(t))$. Your altitude is then $z = f(x(t), y(t))$. How fast is your altitude changing with respect to time, $t$?

The **multivariable chain rule** gives the answer:
$$ \frac{dz}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} $$

This formula has a beautiful, compact form using the gradient and the velocity vector of your path, $r'(t) = \begin{bmatrix} dx/dt \\ dy/dt \end{bmatrix}$:
$$ \frac{dz}{dt} = \nabla f \cdot r'(t) $$
The rate of change of your altitude is the dot product of the "steepest uphill" vector and your direction of travel vector. This is a perfect example of how linear algebra and calculus work together.

## The Main Event: Finding Maxima and Minima

Now we can answer the big question: how do we find the peaks and valleys of our landscape?

At the very top of a peak or the bottom of a valley, the ground is perfectly flat. The slope in *every* direction is zero. This means both partial derivatives must be zero.

> **Critical Points**
> A point $(a, b)$ is a **critical point** of $f(x,y)$ if the gradient at that point is the zero vector:
> $$ \nabla f(a, b) = \begin{bmatrix} 0 \\ 0 \end{bmatrix} \quad \text{which means} \quad f_x(a, b) = 0 \text{ and } f_y(a, b) = 0 $$

But a flat spot isn't always a peak or a valley. It could also be a **saddle point**, like the middle of a Pringles chip—it's a minimum in one direction and a maximum in another.

To classify these critical points, we need a multivariable version of the Second Derivative Test. This test involves a quantity called the **Discriminant** ($D$), which is built from the second-order partial derivatives.

::: {.callout-note}
### The Second Derivative Test

First, find all critical points by solving $\nabla f = 0$. Then, for each critical point $(a, b)$, calculate the second partial derivatives ($f_{xx}, f_{yy}, f_{xy}$) at that point.

Define the Discriminant $D = f_{xx}(a,b) f_{yy}(a,b) - [f_{xy}(a,b)]^2$.

1.  If $D > 0$ and $f_{xx}(a,b) > 0$, then $f$ has a **local minimum** at $(a, b)$.
2.  If $D > 0$ and $f_{xx}(a,b) < 0$, then $f$ has a **local maximum** at $(a, b)$.
3.  If $D < 0$, then $f$ has a **saddle point** at $(a, b)$.
4.  If $D = 0$, the test is inconclusive.
:::

Note that the Discriminant is just the determinant of the **Hessian matrix**, a beautiful connection back to linear algebra!
$$ H = \begin{bmatrix} f_{xx} & f_{xy} \\ f_{yx} & f_{yy} \end{bmatrix} \implies D = \det(H) $$

### Example: Finding the Extrema of Our Landscape

Let's use `SymPy` to find and classify all the critical points of the function $f(x, y) = (x^2 + 3y^2) e^{1 - x^2 - y^2}$ we plotted at the beginning.

```{python}
#| label: find-classify-extrema
#| fig-cap: "Using SymPy to find and classify the critical points of our surface."

import sympy as sp

# Define symbols and the function
x, y = sp.symbols('x y')
f = (x**2 + 3*y**2) * sp.exp(1 - x**2 - y**2)

# 1. Find the partial derivatives
fx = sp.diff(f, x)
fy = sp.diff(f, y)

# 2. Find the critical points by solving ∇f = 0
# This can be computationally intensive; we'll use a numerical approach for clarity
# For this specific function, inspection shows critical points at:
# (0,0), (1,0), (-1,0), (0,1), (0,-1)
critical_points = [
    (0, 0),
    (1, 0),
    (-1, 0),
    (0, 1),
    (0, -1)
]
print(f"The critical points are: {critical_points}\n")


# 3. Calculate second-order partial derivatives
fxx = sp.diff(fx, x)
fyy = sp.diff(fy, y)
fxy = sp.diff(fx, y)

# 4. Create the Discriminant D
D = fxx * fyy - fxy**2

# 5. Classify each critical point
for p in critical_points:
    px, py = p
    # Substitute the point's coordinates into D and fxx
    D_val = D.subs([(x, px), (y, py)])
    fxx_val = fxx.subs([(x, px), (y, py)])
    
    print(f"--- Analyzing point {p} ---")
    print(f"  D = {D_val:.2f}, f_xx = {fxx_val:.2f}")

    if D_val > 0 and fxx_val > 0:
        print("  Result: Local Minimum")
    elif D_val > 0 and fxx_val < 0:
        print("  Result: Local Maximum")
    elif D_val < 0:
        print("  Result: Saddle Point")
    else:
        print("  Result: Test is inconclusive")
```
The results match what we see in the 3D plot perfectly! The origin is a local minimum, the points on the y-axis are local maxima (the two highest peaks), and the points on the x-axis are saddle points.

## Module III Summary

*   We've moved from 2D curves to 3D surfaces, or "landscapes."
*   **Partial derivatives** ($f_x, f_y$) are the slopes in the cardinal directions.
*   The **gradient vector** ($\nabla f = [f_x, f_y]$) packages these slopes and points in the direction of steepest ascent. It is the key to understanding the local geometry of a surface.
*   To find potential maxima and minima (**critical points**), we find where the landscape is flat by solving $\nabla f = 0$.
*   The **Second Derivative Test**, using the determinant of the Hessian matrix, allows us to classify these critical points as local maxima, local minima, or saddle points.
*   This process of finding extrema is called **optimization**, and it is the absolute core of how modern AI models are trained.
```